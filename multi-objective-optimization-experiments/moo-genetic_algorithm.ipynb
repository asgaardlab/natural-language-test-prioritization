{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af29f3ba-72ba-4cae-9e85-e90ffa89519c",
   "metadata": {},
   "source": [
    "### Multi-objective optimization using NSGA-II for the prioritization of manual test cases in natural language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5526d694-1bcc-4e6f-8949-ff4e5eb67602",
   "metadata": {},
   "source": [
    "We performed two main experiments:\n",
    "* **Without feature usage**: our objectives functions are *number of covered features* (maximize) and *test execution time* (minimize)\n",
    "* **With feature usage**: our objectives functions are *number of covered highly-used game features* (maximize) and *test execution time* (minimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fdceb8d-3bcd-4a84-8fd2-099fa2d55b47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys  \n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from statistics import mean, median\n",
    "from scipy import stats\n",
    "from scipy.stats import mannwhitneyu\n",
    "from cliffs_delta import cliffs_delta\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.problems.functional import FunctionalProblem\n",
    "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "from pymoo.operators.sampling.rnd import PermutationRandomSampling\n",
    "from pymoo.operators.crossover.ox import OrderCrossover\n",
    "from pymoo.operators.mutation.inversion import InversionMutation\n",
    "from pymoo.core.problem import Problem\n",
    "from pymoo.core.population import Population\n",
    "from pymoo.core.callback import Callback\n",
    "from pymoo.core.termination import Termination\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eff8290c-6427-4fb8-a447-910dcb328b45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import modules (py scripts) with needed functions and classes\n",
    "sys.path.insert(0, '/utils')\n",
    "sys.path.insert(0, '/optimization')\n",
    "\n",
    "from utils import utils\n",
    "from optimization import optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23c8fff-2dd6-498c-baf3-f742ea34560c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load data: test cases with test labels (covered game features) and execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc87f78-9ca8-4026-a4ab-caff2c56f475",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases_complete = utils.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa997863-c869-4ebb-aaa9-df14c758bd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information from test cases\n",
    "[feature_coverage_test_suite, execution_time_test_suite, mapping_test_key_test_suite,\n",
    " feature_test_id_relation, total_execution_time_test_suite, number_test_cases] = utils.get_test_data_info(test_cases_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4748f5-8a92-4b22-b5a5-615cb0289633",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_full_suite = []\n",
    "\n",
    "for key,value in feature_coverage_test_suite.items():\n",
    "    for elem in value:\n",
    "        if elem not in features_full_suite:\n",
    "            features_full_suite.append(elem)\n",
    "\n",
    "num_features_full_suite = len(features_full_suite)\n",
    "print(\"There are {num} features being covered in test cases\".format(num=num_features_full_suite))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bc8a6f-9d7f-4c87-a79d-d425b7e78fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of features tested in each test case\n",
    "num_feat_tested = []\n",
    "\n",
    "for testcase in feature_coverage_test_suite:\n",
    "    num_feat_tested.append(len(feature_coverage_test_suite[testcase]))\n",
    "    \n",
    "print(\"There are {count} features in each test case, on average.\".format(count=mean(num_feat_tested)))\n",
    "print(\"There is a median {count} features in each test case.\".format(count=median(num_feat_tested)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ead56b-0158-4f73-be66-20104d82a70c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Get feature usage and basic usage stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813cf1d2-19cb-4930-9706-dd5f97899e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read feature usage data (this data was collected from Snowflake at Prodigy and saved with the pickle format)\n",
    "num_uses_features = pd.read_pickle('FEATURE_USAGE_DATA_PATH')\n",
    "print(\"Number of features: {num}\".format(num=len(num_uses_features)))\n",
    "num_uses_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc90e3f-ce12-444a-a4ab-477b6d0b4840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build mapping (from Snowflake data to testing data)\n",
    "mapping_dict = utils.build_feature_mapping()\n",
    "\n",
    "# Check features that are not in the mapping (e.g., corrupted data from game data events). Use this to check if there is corrupted usage data that needs to be fixed\n",
    "utils.remove_features_mapping(mapping_dict, num_uses_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccaf645-367e-498a-ad0b-ad0bd2e46fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sort by usage and get some basic stats\n",
    "num_used_features = num_uses_features.sort_values(by='Number of uses', ascending=False)\n",
    "display(num_used_features.head())\n",
    "\n",
    "most_used_features_test_suite = num_uses_features['Feature'].to_list()\n",
    "print(\"Number of features: {num}\".format(num=len(most_used_features_test_suite)))\n",
    "\n",
    "# Generate boxplot\n",
    "px.box(num_uses_features['Number of uses'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522990b1-bfc6-443d-ae0e-07279ff2d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get median usage and set median_num_features\n",
    "feat_usage_list = num_uses_features['Number of uses'].to_list()\n",
    "\n",
    "median_usage = median(feat_usage_list)\n",
    "counter_feat = 1\n",
    "cum_usage = 0\n",
    "for current_usage in feat_usage_list:\n",
    "    cum_usage += current_usage\n",
    "    if current_usage <= median_usage:\n",
    "        break\n",
    "    counter_feat += 1\n",
    "\n",
    "print(\"Cumulative usage up to median usage: {cum_usage}\".format(cum_usage=cum_usage))\n",
    "\n",
    "cum_median_usage_perc = cum_usage/(sum(feat_usage_list))\n",
    "print(\"Percentage of cumulative usage up to median usage: {cum_median_usage_perc}\".format(cum_median_usage_perc=cum_median_usage_perc))\n",
    "print(\"Corresponding number of features up to median usage: {median_num_features}\".format(median_num_features=counter_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a69b887c-e7fe-4eef-9822-b03513f2ccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert feature names from game data in Snowflake to standard names in the testing side\n",
    "most_used_features_test_suite_testing_std = [mapping_dict[x] for x in most_used_features_test_suite] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412d620d-1fd0-42d9-8f74-a788041bf53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some game data events lead to the same testing feature (i.e., there are duplicates in most_used_features_test_suite_testing_std). Below, we remove them keeping the first\n",
    "most_used_features_test_suite_testing_std = list(dict.fromkeys(most_used_features_test_suite_testing_std))\n",
    "print(\"Now there are {num} features in the set of most used features\".format(num=len(most_used_features_test_suite_testing_std)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a437028-34bd-486d-9ee9-56988bb5e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List with ranking position of the most used features\n",
    "most_used_features_rank = {}\n",
    "counter = 1\n",
    "for feat in most_used_features_test_suite_testing_std:\n",
    "    most_used_features_rank[feat] = counter\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf0bdf0d-1874-444d-b0fe-4792f11eea09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get num. of times each feature is covered (i.e., number of test cases where each feature appears)\n",
    "feature_counter = {}\n",
    "for key,value in feature_coverage_test_suite.items():\n",
    "    for feat in value:\n",
    "        if feat not in feature_counter:\n",
    "            feature_counter[feat] = 1\n",
    "        else:\n",
    "            feature_counter[feat] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7a4f64-0904-42a5-b89d-fa3db5edef99",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Define objective/fitness functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcd7ccf2-c02e-466e-9bd2-1cb2d1c8b966",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute cumulative feature coverage for a specific test execution order and feature per-feature coverage threshold\n",
    "def get_cum_feature_coverage_test_suite(order, feature_counter, thresh):\n",
    "    cum_feature_count = 0\n",
    "    cum_feature_count_array = []\n",
    "    features_covered = []\n",
    "    feature_counter_order = dict.fromkeys(feature_counter.keys(),0)\n",
    "\n",
    "    for test_case_id in order:\n",
    "        features = feature_coverage_test_suite[test_case_id]\n",
    "\n",
    "        # Update dict with counter\n",
    "        for feat in features:\n",
    "            feature_counter_order[feat] = feature_counter_order[feat] + 1\n",
    "            \n",
    "            # Check if feature meets threshold to be considered covered\n",
    "            if feature_counter_order[feat] >= math.floor(feature_counter[feat]*thresh):\n",
    "                if feat not in features_covered:\n",
    "                    features_covered.append(feat)\n",
    "                    cum_feature_count = cum_feature_count + 1\n",
    "\n",
    "        cum_feature_count_array.append(cum_feature_count)\n",
    "\n",
    "    # Define arrays for test case (X axis) versus cumulative feature count (Y axis)\n",
    "    x = np.array([i for i in range(len(order))])\n",
    "    y = cum_feature_count_array\n",
    "    \n",
    "    # Compute area using trapezoidal rule\n",
    "    area = np.trapz(y=y, x=x)\n",
    "    \n",
    "    # Compute optimal area\n",
    "    optimal_area = np.trapz(y=np.array([num_features_full_suite]*len(x)), x=x)\n",
    "\n",
    "    # Normalize area\n",
    "    norm_area = area/optimal_area\n",
    "\n",
    "    return norm_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcc747e4-3c1d-415d-ba67-f08cb6df42c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute cumulative execution time for a specific test execution order\n",
    "def get_cum_execution_time_test_suite(order):\n",
    "    cum_execution_time = 0\n",
    "    cum_execution_time_array = []\n",
    "\n",
    "    for test_case_id in order:\n",
    "        exec_time = execution_time_test_suite[test_case_id]\n",
    "        cum_execution_time = cum_execution_time + exec_time\n",
    "        cum_execution_time_array.append(cum_execution_time)\n",
    "        \n",
    "    x = np.array([i for i in range(len(order))])\n",
    "    y = cum_execution_time_array\n",
    "    \n",
    "    # Compute area using trapezoidal rule\n",
    "    area = np.trapz(y=y, x=x)\n",
    "\n",
    "    # Compute optimal area\n",
    "    optimal_area = np.trapz(y=np.array([total_execution_time_test_suite]*len(x)), x=x)\n",
    "    \n",
    "    # Normalize area\n",
    "    norm_area = area/optimal_area\n",
    "\n",
    "    return norm_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "718483c1-084f-452e-b327-ae490b0c3b94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute ranking similarity score for a specific test execution order using 'feature usage' as relevance for features\n",
    "def get_ranking_score_full_suite(order, true_relev, scaler_ndcg):\n",
    "    global most_used_features_rank\n",
    "    \n",
    "    list_tested_features = []\n",
    "    \n",
    "    for test_case_id in order:\n",
    "        features = feature_coverage_test_suite_reduced[test_case_id] \n",
    "        for feat in features:\n",
    "                \n",
    "            if feat not in list_tested_features:\n",
    "                list_tested_features.append(feat)\n",
    "        \n",
    "    tested_feat_relev = {}\n",
    "    for feat in list_tested_features:\n",
    "        # Use usage ranking to find relevance for the currently tested feature\n",
    "        tested_feat_relev[feat] = most_used_features_rank[feat]\n",
    "\n",
    "    score_relev = list(tested_feat_relev.values())\n",
    "    \n",
    "    # Compute NDCG\n",
    "    score = ndcg_score(np.asarray([true_relev]),np.asarray([score_relev]))\n",
    "    scaled_score = scaler_ndcg.transform(np.asarray(score).reshape(-1, 1))[0][0]\n",
    "    return scaled_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb8e5b6-2749-4b92-8932-0106434d1175",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Modify NSGA-II to use T-test and mutual dominance rate as stopping criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e64a1f3-709c-47eb-bdad-e883ee2f0ce5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using callback and custom termination criteria\n",
    "class MyCallback(Callback):\n",
    "    \n",
    "    def __init__(self, mdr_limit) -> None:\n",
    "        super().__init__()\n",
    "        self.data[\"terminate\"] = False\n",
    "        self.data[\"index\"] = 0\n",
    "        self.data[\"last_gen_solution\"] = []\n",
    "        self.data[\"current_gen_solution\"] = []\n",
    "        self.data[\"last_gen_objectives\"] = []\n",
    "        self.data[\"current_gen_objectives\"] = []\n",
    "        self.data[\"last_gen_t_test\"] = False\n",
    "        self.data[\"current_gen_t_test\"] = False\n",
    "        self.data[\"count_last_mdr\"] = 0\n",
    "        self.data[\"count_last_t_test\"] = 0\n",
    "        self.data[\"MDR_LIMIT\"] = mdr_limit\n",
    "        \n",
    "    def get_mutual_dom_rate(self, gen_1_obj, gen_2_obj):\n",
    "        counter = 0\n",
    "        found = False\n",
    "        for obj_values_gen_1 in gen_1_obj:\n",
    "            for obj_values_gen_2 in gen_2_obj:\n",
    "                if not found:\n",
    "                    if all(obj_values_gen_2[index] >= obj_values_gen_1[index] for index in range(len(obj_values_gen_2))):\n",
    "                        if any(obj_values_gen_2[index] > obj_values_gen_1[index] for index in range(len(obj_values_gen_2))):\n",
    "                            counter += 1\n",
    "                            found = True\n",
    "            found = False\n",
    "            \n",
    "        return counter\n",
    "\n",
    "    def notify(self, algorithm):\n",
    "\n",
    "        if self.data[\"index\"] == 0:\n",
    "            self.data[\"last_gen_solution\"] = algorithm.opt\n",
    "            self.data[\"index\"] = 1\n",
    "            obj_results = algorithm.opt.get(\"F\").T\n",
    "            self.data[\"last_gen_objectives\"] = obj_results\n",
    "\n",
    "        else:\n",
    "            self.data[\"current_gen_solution\"] = algorithm.opt\n",
    "            \n",
    "            # Transpose objective results and iterate through results for all objectives\n",
    "            obj_results = algorithm.opt.get(\"F\").T\n",
    "            self.data[\"current_gen_objectives\"] = obj_results\n",
    "\n",
    "            # Compute t-test for each objective\n",
    "            t_test_res_list = []\n",
    "            for index in range(len(self.data[\"current_gen_objectives\"])):\n",
    "                single_obj_values_current = self.data[\"current_gen_objectives\"][index]\n",
    "                single_obj_values_last = self.data[\"last_gen_objectives\"][index]\n",
    "                t_test_res = stats.ttest_ind(single_obj_values_current, single_obj_values_last)[1] # get only p-value\n",
    "                t_test_res_list.append(t_test_res)\n",
    "            \n",
    "            # Get mutual dominance rates\n",
    "            dom_last_current = self.get_mutual_dom_rate(self.data[\"last_gen_objectives\"].T, self.data[\"current_gen_objectives\"].T)\n",
    "            dom_current_last = self.get_mutual_dom_rate(self.data[\"current_gen_objectives\"].T, self.data[\"last_gen_objectives\"].T)\n",
    "            mdr = (dom_last_current/len(self.data[\"last_gen_objectives\"].T)) - (dom_current_last/len(self.data[\"current_gen_objectives\"].T))\n",
    "            \n",
    "            # Check is mdr is within range and increase counter; otherwise, reset counter\n",
    "            if abs(mdr) < self.data[\"MDR_LIMIT\"]:\n",
    "                self.data[\"count_last_mdr\"] += 1\n",
    "            else:\n",
    "                self.data[\"count_last_mdr\"] = 0\n",
    "                \n",
    "            # Check is t-test for all objectives is above 0.05 and increase counter; otherwise, reset counter\n",
    "            if all(i > 0.05 for i in t_test_res_list):\n",
    "                self.data[\"count_last_t_test\"] += 1\n",
    "            else:\n",
    "                self.data[\"count_last_t_test\"] = 0\n",
    "                \n",
    "            # Check if mdr and t-test meet criteria for 5 generations\n",
    "            if (self.data[\"count_last_mdr\"] >= 5) and (self.data[\"count_last_t_test\"] >= 5):\n",
    "                self.data[\"terminate\"] = True\n",
    "\n",
    "            # Update last generation\n",
    "            self.data[\"last_gen_solution\"] = self.data[\"current_gen_solution\"]\n",
    "            self.data[\"last_gen_objectives\"] = self.data[\"current_gen_objectives\"]\n",
    "            self.data[\"last_gen_t_test\"] = self.data[\"current_gen_t_test\"]\n",
    "            self.data[\"last_gen_mdr\"] = mdr\n",
    "\n",
    "\n",
    "class MyTermination(Termination):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "    def terminate(self):\n",
    "        self.force_termination = True\n",
    "   \n",
    "    def update(self, algorithm):\n",
    "        \"\"\"\n",
    "        Provide the termination criterion a current status of the algorithm to update the perc.\n",
    "        Parameters\n",
    "        ----------\n",
    "        algorithm : object\n",
    "            The algorithm object which is used to determine whether a run has terminated.\n",
    "        \"\"\"\n",
    "        \n",
    "        if my_callback.data[\"terminate\"]:\n",
    "            self.terminate()\n",
    "        \n",
    "        if self.force_termination:\n",
    "            progress = 1.0\n",
    "        else:\n",
    "            progress = self._update(algorithm)\n",
    "            assert progress >= 0.0\n",
    "\n",
    "        self.perc = progress\n",
    "        return self.perc\n",
    "    \n",
    "    def _update(self, algorithm):\n",
    "        return 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f094f7-4823-4549-b931-7bd6f230150f",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-22.08",
   "language": "python",
   "name": "rapids-22.08"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
