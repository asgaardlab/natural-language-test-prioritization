{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11e81441-b536-44f8-b933-9456266c5d72",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experiments with zero-shot classification techniques to classify manual test cases (i.e., textual descriptions of test cases) into the game features that they cover.\n",
    "\n",
    "We experiment with the following models/techniques and their combinations:\n",
    "\n",
    "* BartLargeMNLI - [facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli)\n",
    "* CrossEncoderNLI - [cross-encoder/nli-distilroberta-base](https://huggingface.co/cross-encoder/nli-distilroberta-base)\n",
    "* LatentEmb - [latent-embeddings](https://joeddav.github.io/blog/2020/05/29/ZSL.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f80a4bbb-97eb-4db0-894b-e90c855b5b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statistics import median, mean\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec, Phrases, KeyedVectors\n",
    "import fasttext\n",
    "from scipy import spatial\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import multilabel_confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import warnings\n",
    "from IPython.display import display, HTML\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d917a7-6eb9-4b14-9069-935a8f535a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook configurations\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7893294e-870f-427d-a1ec-14189cb9de00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules with different classification methods\n",
    "from zero_shot_nli import run_zero_shot_nli\n",
    "from zero_shot_nli_metrics_per_class import run_zero_shot_nli_metrics_per_class\n",
    "from zero_shot_latent_embedding import run_zero_shot_latent_emb\n",
    "from baseline import run_baseline\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c775d9c5-b8af-4a8d-abdb-2287cdaf92e1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50213011-9d3b-4e5d-9a64-ae32b284d764",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Load and pre-process labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df3f82b-ecb2-4d90-837d-90e83762012f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load labeled data\n",
    "labeled_test_cases_df = utils.read_data()\n",
    "labeled_test_cases_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee51372-f123-4f71-9e06-bd7d35c06cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process data\n",
    "(test_case_name_df, test_case_name_obj_df) = utils.preprocess_data(labeled_test_cases_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90708d71-6257-4bf3-b9c4-9ee13cae2f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of unique labels (game features)\n",
    "unique_labels = []\n",
    "for index,row in test_case_name_df.iterrows():\n",
    "    labels = row['labels']\n",
    "    for lab in labels:\n",
    "        if lab not in unique_labels:\n",
    "            unique_labels.append(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbef15c0-5702-47b0-8345-2a201e81ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict with counter of unique labels\n",
    "unique_labels_count = dict.fromkeys(unique_labels,0)\n",
    "for index,row in test_case_name_df.iterrows():\n",
    "    labels = row['labels']\n",
    "    for lab in labels:\n",
    "        unique_labels_count[lab] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf57646-84d3-4f95-ba1c-7bb379a358a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg number of unique labels\n",
    "mean_label_counter = mean(list(unique_labels_count.values()))\n",
    "print(\"There are on average {count} unique labels.\".format(count=mean_label_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dffdfe-c650-41d0-a244-b42cbe235c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels (game features)\n",
    "candidate_label_file = \"INSERT_DIR_OF_LIST_OF_GAME_FEATURES\"\n",
    "candidate_labels = candidate_label_file.read().splitlines()\n",
    "print(\"There are {count} candidate labels.\".format(count=len(candidate_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9677f1-0fea-4f5d-a1ec-93dbcc5839b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace dash by space in candidate labels with more than one word (achieves better performance)\n",
    "candidate_labels_mod = []\n",
    "for elem in candidate_labels:\n",
    "    res = ' '.join(elem.split('-'))\n",
    "    candidate_labels_mod.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6f0da06-0d8d-497b-95cd-48a0d4caa553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Mlflow experiment dir\n",
    "experiment_dir = \"INSERT_DIR_TO_RECORD_EXPERIMENTS_WITH_MLFLOW\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c73f4b9-8afb-4c40-8ecf-de9dad49ea5b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2adb4b7-f6f1-4342-9a98-d34f5008d19b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f874384c-faf5-4938-9937-1779fa2c5f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define name and description of experiment\n",
    "experiment_name = \"Baseline experiment - Test case name and objective\"\n",
    "experiment_active = mlflow.set_experiment(experiment_name)\n",
    "experiment_id = experiment_active.experiment_id\n",
    "MlflowClient().set_experiment_tag(experiment_id, \n",
    "     \"mlflow.note.content\",\"Evaluate keyword-based approach to classify test cases (with test case name and objective).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38186ee-7000-4a01-828c-2c9a71174693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases represented by name\n",
    "run_name = \"Test case name\"\n",
    "run_baseline(test_case_name_df, candidate_labels, candidate_labels_mod, experiment_name, run_name, experiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44960c69-46d8-4909-a0c4-aefddbe4efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases represented by name + objective\n",
    "run_name = \"Test case name + objective\"\n",
    "run_baseline(test_case_name_obj_df, candidate_labels, candidate_labels_mod, experiment_name, run_name, experiment_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0071a9f-882b-4799-b31a-750fd7d6e43a",
   "metadata": {},
   "source": [
    "### Experiments with individual zero-shot techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e96a0aa-98f4-47cf-84b6-71d8e59651b2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### BartLargeMNLI - [facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd6817-a555-44e4-8f64-bdbaa83d68aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define name and description of experiment\n",
    "experiment_name = \"BartLargeMNLI - Test case name and objective\"\n",
    "experiment_active = mlflow.set_experiment(experiment_name)\n",
    "experiment_id = experiment_active.experiment_id\n",
    "MlflowClient().set_experiment_tag(experiment_id, \n",
    "     \"mlflow.note.content\",\"Evaluate BartLargeMNLI to classify test cases (with test case name and objective).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0ad8b0-af1a-4090-b9ce-941b9e62b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load zero-shot classifier from the HF pipeline - set device=0 to use GPU for faster inference\n",
    "zero_shot_nli_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f92b9f-6713-4a98-a531-19d692581212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classifier - considering test case as test case name\n",
    "run_name = \"Test case name\"\n",
    "run_zero_shot_nli(zero_shot_nli_classifier, candidate_labels, test_case_name_df, experiment_name, run_name, experiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26af7ae7-ab8c-4795-a9c9-fc480d6c13bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classifier - considering test case as test case name + test case objective\n",
    "run_name = \"Test case name + objective\"\n",
    "run_zero_shot_nli(zero_shot_nli_classifier, candidate_labels, test_case_name_obj_df, experiment_name, run_name, experiment_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c6e593-8422-4eb6-af98-3fa90c37cf3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### CrossEncoderNLI - [cross-encoder/nli-distilroberta-base](https://huggingface.co/cross-encoder/nli-distilroberta-base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a916023-bbda-411e-be12-4b0e526590f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define name and description of experiment\n",
    "experiment_name = \"CrossEncoderNLI - Test case name and objective\"\n",
    "experiment_active = mlflow.set_experiment(experiment_name)\n",
    "experiment_id = experiment_active.experiment_id\n",
    "MlflowClient().set_experiment_tag(experiment_id, \n",
    "     \"mlflow.note.content\",\"Evaluate CrossEncoderNLI to classify test cases (with test case name and objective).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdbcc9c-e878-4726-845e-68e12bbd06eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load zero-shot classifier from the HF pipeline - set device=0 to use GPU for faster inference\n",
    "zero_shot_nli_cross_enc_classifier = pipeline(\"zero-shot-classification\", model='cross-encoder/nli-distilroberta-base', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c98d49-20fb-41d9-8b97-45e92118a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classifier - considering test case as test case name\n",
    "run_name = \"Test case name\"\n",
    "run_zero_shot_nli(zero_shot_nli_cross_enc_classifier, candidate_labels, test_case_name_df, experiment_name, run_name, experiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f438d39-0e63-4547-a688-92ddf6a8c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classifier - considering test case as test case name + test case objective\n",
    "run_name = \"Test case name + objective\"\n",
    "run_zero_shot_nli(zero_shot_nli_cross_enc_classifier, candidate_labels, test_case_name_obj_df, experiment_name, run_name, experiment_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151992a1-dbe8-4aca-a35e-f5aca7bbf93d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### LatentEmb - [latent-embeddings](https://joeddav.github.io/blog/2020/05/29/ZSL.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d32469-8ba9-43a5-83c1-1dd791fb26cf",
   "metadata": {},
   "source": [
    "We experiment with Wor2vec, Fasttext, and Glove embedding models together with the SBERT sentence embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b682b582-c47a-4585-9014-f24c968ad56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define name and description of experiment\n",
    "experiment_name = \"Zero-shot Latent embeddings - Word embbeding models\"\n",
    "experiment_active = mlflow.set_experiment(experiment_name)\n",
    "experiment_id = experiment_active.experiment_id\n",
    "MlflowClient().set_experiment_tag(experiment_id, \n",
    "     \"mlflow.note.content\",\"Evaluate different word embedding models for zero-shot with latent embeddings approach\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbe94b2-43d0-47f7-967a-4a3222d67665",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_name = 'sentence-t5-large'\n",
    "sbert_model = SentenceTransformer(sbert_name, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade6b71c-2f5a-49e7-8f63-66e9e6d97584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordinary_least_squares_lr(\n",
    "    X: torch.Tensor, Y: torch.Tensor, alpha: float = 0) -> torch.Tensor:\n",
    "    \"\"\"Computes ordinary least squares\n",
    "    For more information on the derivation of the closed-form expression,\n",
    "    check it the Wikipedia page here:\n",
    "    https://en.wikipedia.org/wiki/Ordinary_least_squares#Matrix/vector_formulation\n",
    "    In brief: we find a matrix, w, that transforms X to Y according to:\n",
    "    Y = Xw\n",
    "    (X.T X)^-1 X.T Y = [(X.T X)^-1 X.T X]w\n",
    "    w = (X.T X + alpha*I)^-1 X.T Y\n",
    "    where I is the identity matrix and alpha is the amount of regularization.\n",
    "    alpha = 0 is equivalent to OLS (ordinary least squares)\n",
    "    alpha >= 0 is ridge regression / l2 regularization\n",
    "    \"\"\"\n",
    "    X_norm = F.normalize(X, p=2, dim=1)\n",
    "    Y_norm = F.normalize(Y, p=2, dim=1)\n",
    "    I = torch.eye(X_norm.shape[1])\n",
    "\n",
    "    inner = torch.matmul(X_norm.T, X_norm) + alpha * I\n",
    "    # Z = torch.linalg.inv(inner)\n",
    "    Z = torch.inverse(inner)\n",
    "    Z = torch.matmul(Z, X_norm.T)\n",
    "    w = torch.matmul(Z, Y_norm)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab4450b-3bb5-4e88-a2f4-bed33c3a2eb6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Zero-shot Latent embeddings - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a718cd4b-59c1-4c48-82b2-63c8627da8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word2vec pre-trained model\n",
    "w2v_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b1d661-372c-4591-8d85-b0949632e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_words_pretrained = w2v_model.index_to_key[:20000]\n",
    "print(\"Len of topk word vector\", len(topk_words_pretrained))\n",
    "\n",
    "# Remove stopwords\n",
    "topk_words_pretrained = [x for x in topk_words_pretrained if x not in stopwords.words('english')]\n",
    "\n",
    "# Remove punctuations\n",
    "topk_words_pretrained = [x for x in topk_words_pretrained if ( (x not in string.punctuation) or (x[0] not in string.punctuation) )]\n",
    "\n",
    "# Remove single letters/digits\n",
    "topk_words_pretrained = [x for x in topk_words_pretrained if len(x) > 1]\n",
    "\n",
    "# Remove any remaining number\n",
    "topk_words_pretrained = [x for x in topk_words_pretrained if not x.isdigit()]\n",
    "\n",
    "print(\"Len of topk word vector after filtering\", len(topk_words_pretrained))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2758e49f-ff2f-4b9a-922b-ac723bc07277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get w2v embeddings\n",
    "w2v_emb_vectors = []\n",
    "for word in topk_words_pretrained:\n",
    "    w2v_emb_vectors.append(w2v_model.get_vector(word))\n",
    "    \n",
    "w2v_emb_vectors = np.array(w2v_emb_vectors)\n",
    "w2v_emb_vectors = torch.tensor(w2v_emb_vectors)\n",
    "print(\"Len of w2v embedding vector list\", len(w2v_emb_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f0f370-afb0-4051-8492-c71eb6c3504b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get SBERT embeddings for the same set of words\n",
    "sbert_emb_vectors = []\n",
    "for word in topk_words_pretrained:\n",
    "    sbert_emb_vectors.append(sbert_model.encode(word))\n",
    "    \n",
    "sbert_emb_vectors = np.array(sbert_emb_vectors)\n",
    "sbert_emb_vectors = torch.tensor(sbert_emb_vectors)\n",
    "print(\"Len of sbert embedding vector list\", len(sbert_emb_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdbe95b-1c48-4dbc-97fb-ac7874972a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute transfer matrix\n",
    "transfer_matrix = ordinary_least_squares_lr(sbert_emb_vectors, w2v_emb_vectors, alpha=0)\n",
    "print(transfer_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f48636c-686d-4969-be56-ff2d384d9401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings of candidate labels\n",
    "candidate_label_embeddings = sbert_model.encode(candidate_labels_mod)\n",
    "print(candidate_label_embeddings.shape)\n",
    "\n",
    "# Covert to tensor\n",
    "candidate_label_embeddings = torch.tensor(candidate_label_embeddings)\n",
    "print(candidate_label_embeddings.shape)\n",
    "\n",
    "# Apply linear transformation\n",
    "candidate_label_embeddings_transformed = torch.mm(candidate_label_embeddings, transfer_matrix)\n",
    "print(candidate_label_embeddings_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad7a8e4-87c3-4f28-84f6-ea84118824cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classifier - considering test case as test case name\n",
    "run_name = 'Test case name - ' + 'Word2Vec + ' + sbert_name\n",
    "run_zero_shot_latent_emb(test_case_name_df, candidate_labels_mod, candidate_label_embeddings_transformed,\n",
    "                         sbert_model, transfer_matrix, experiment_name, run_name, experiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cba818-71c3-4552-8f8b-7e0c52afb28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classifier - considering test case as test case name + test case objective\n",
    "run_name = 'Test case name + objective - ' + 'Word2Vec + ' + sbert_name\n",
    "run_zero_shot_latent_emb(test_case_name_obj_df, candidate_labels_mod, candidate_label_embeddings_transformed,\n",
    "                         sbert_model, transfer_matrix, experiment_name, run_name, experiment_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b654db0-1820-4d69-909d-b0ba859922de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Zero-shot Latent embeddings - Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fca459-b388-432b-86d3-af5aedc325c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "fasttext_model = fasttext.load_model(\"INSERT_PATH_OF_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e67616-096d-4da7-b604-482afb56229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_words_pretrained = fasttext_model.get_words()[:20000]\n",
    "print(\"Len of topk word vector\", len(topk_words_pretrained))\n",
    "\n",
    "# Remove stopwords\n",
    "topk_words_pretrained = [x for x in topk_words_pretrained if x not in stopwords.words('english')]\n",
    "\n",
    "# Remove punctuations\n",
    "topk_words_pretrained = [x for x in topk_words_pretrained if ( (x not in string.punctuation) or (x[0] not in string.punctuation) )]\n",
    "\n",
    "# Remove single letters/digits\n",
    "topk_words_pretrained = [x for x in topk_words_pretrained if len(x) > 1]\n",
    "\n",
    "# Remove any remaining number\n",
    "topk_words_pretrained = [x for x in topk_words_pretrained if not x.isdigit()]\n",
    "\n",
    "print(\"Len of topk word vector after filtering\", len(topk_words_pretrained))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a88564c-a4c6-410e-a457-3441cde21625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fasttext embeddings\n",
    "ft_emb_vectors = []\n",
    "for word in topk_words_pretrained:\n",
    "    ft_emb_vectors.append(fasttext_model.get_word_vector(word))\n",
    "    \n",
    "ft_emb_vectors = np.array(ft_emb_vectors)\n",
    "ft_emb_vectors = torch.tensor(ft_emb_vectors)\n",
    "print(\"Len of fasttext embedding vector list\", len(ft_emb_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afb3d2f-73b0-4262-a659-7d44a6000e95",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get SBERT embeddings for the same set of words\n",
    "sbert_emb_vectors = []\n",
    "for word in topk_words_pretrained:\n",
    "    sbert_emb_vectors.append(sbert_model.encode(word))\n",
    "    \n",
    "sbert_emb_vectors = np.array(sbert_emb_vectors)\n",
    "sbert_emb_vectors = torch.tensor(sbert_emb_vectors)\n",
    "print(\"Len of sbert embedding vector list\", len(sbert_emb_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0863359c-5eb8-409f-8e85-6d5e95b0a7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute transfer matrix\n",
    "transfer_matrix = ordinary_least_squares_lr(sbert_emb_vectors, ft_emb_vectors, alpha=0)\n",
    "print(transfer_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e7ca70-f415-42c5-b48b-45f6673ab589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings of candidate labels\n",
    "candidate_label_embeddings = sbert_model.encode(candidate_labels_mod)\n",
    "print(candidate_label_embeddings.shape)\n",
    "\n",
    "# Covert to tensor\n",
    "candidate_label_embeddings = torch.tensor(candidate_label_embeddings)\n",
    "print(candidate_label_embeddings.shape)\n",
    "\n",
    "# Apply linear transformation\n",
    "candidate_label_embeddings_transformed = torch.mm(candidate_label_embeddings, transfer_matrix)\n",
    "print(candidate_label_embeddings_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e5cac-3191-4daf-86fc-cddbf091bf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classifier - considering test case as test case name\n",
    "run_name = 'Test case name - ' + 'Fasttext + ' + sbert_name\n",
    "run_zero_shot_latent_emb(test_case_name_df, candidate_labels_mod, candidate_label_embeddings_transformed,\n",
    "                         sbert_model, transfer_matrix, experiment_name, run_name, experiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c173b23f-cd78-48b3-8b20-50656aa1bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classifier - considering test case as test case name + test case objective\n",
    "run_name = 'Test case name + objective - ' + 'Fasttext + ' + sbert_name\n",
    "run_zero_shot_latent_emb(test_case_name_obj_df, candidate_labels_mod, candidate_label_embeddings_transformed,\n",
    "                         sbert_model, transfer_matrix, experiment_name, run_name, experiment_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fdde9c-195a-49fa-8e74-4439c7157791",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Zero-shot Latent embeddings - Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b7e0b9-f5bf-44f2-9235-6fe436c4cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary with word embeddings from Glove\n",
    "embeddings_index = {}\n",
    "f = open('INSERT_PATH_OF_MODEL','r',encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    vector = np.asarray([float(val) for val in values[1:]])\n",
    "    embeddings_index[word] = vector\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62f6050-dd2c-4bdd-baa0-7aa17232cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get glove embeddings (using the same 'topk_words_pretrained' as before since we cannot get word frequency from glove)\n",
    "glove_emb_vectors = []\n",
    "sbert_emb_vectors = []\n",
    "\n",
    "for word in topk_words_pretrained:\n",
    "    try:\n",
    "        glove_emb_vectors.append(embeddings_index[word])\n",
    "        sbert_emb_vectors.append(sbert_model.encode(word))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "glove_emb_vectors = np.array(glove_emb_vectors)\n",
    "glove_emb_vectors = torch.tensor(glove_emb_vectors)\n",
    "\n",
    "# Convert from float64 (double) to float\n",
    "glove_emb_vectors = glove_emb_vectors.type(torch.float32) \n",
    "print(\"Len of glove embedding vector list\", len(glove_emb_vectors))\n",
    "\n",
    "sbert_emb_vectors = np.array(sbert_emb_vectors)\n",
    "sbert_emb_vectors = torch.tensor(sbert_emb_vectors)\n",
    "print(\"Len of sbert embedding vector list\", len(sbert_emb_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5736e23-57b0-4bd7-9943-11b4c3e428de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute transfer matrix\n",
    "transfer_matrix = ordinary_least_squares_lr(sbert_emb_vectors, glove_emb_vectors, alpha=0)\n",
    "print(transfer_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50a0216-2b0f-4508-954b-4f1f9c2cb3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings of candidate labels\n",
    "candidate_label_embeddings = sbert_model.encode(candidate_labels_mod)\n",
    "print(candidate_label_embeddings.shape)\n",
    "\n",
    "# Covert to tensor\n",
    "candidate_label_embeddings = torch.tensor(candidate_label_embeddings)\n",
    "print(candidate_label_embeddings.shape)\n",
    "\n",
    "# Apply linear transformation\n",
    "candidate_label_embeddings_transformed = torch.mm(candidate_label_embeddings, transfer_matrix)\n",
    "print(candidate_label_embeddings_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd91ead-cad4-489e-b6a6-0c7092c6b4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classifier - considering test case as test case name\n",
    "run_name = 'Test case name - ' + 'Glove + ' + sbert_name\n",
    "run_zero_shot_latent_emb(test_case_name_df, candidate_labels_mod, candidate_label_embeddings_transformed,\n",
    "                         sbert_model, transfer_matrix, experiment_name, run_name, experiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e80dd-db6d-4917-918a-aa023e96a88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classifier - considering test case as test case name + test case objective\n",
    "run_name = 'Test case name + objective - ' + 'Glove + ' + sbert_name\n",
    "run_zero_shot_latent_emb(test_case_name_obj_df, candidate_labels_mod, candidate_label_embeddings_transformed,\n",
    "                         sbert_model, transfer_matrix, experiment_name, run_name, experiment_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bc933f-ccb8-4f59-a6dc-31617fa769d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experiments with **ensembles** of individual zero-shot techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070390d4-8a5f-4359-91f8-28c241bd8aa6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Load individual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec3594d-361d-40ad-baa8-597e9e505390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models for latent embeddings approach\n",
    "sbert_name = 'sentence-t5-large'\n",
    "sbert_model = SentenceTransformer(sbert_name, device='cuda')\n",
    "\n",
    "# Compute transfer matrix with best word embedding model (word2vec)\n",
    "transfer_matrix = ordinary_least_squares_lr(sbert_emb_vectors, w2v_emb_vectors, alpha=0)\n",
    "print(transfer_matrix.shape)\n",
    "\n",
    "# Compute candidate label embeddings with linear transformation\n",
    "candidate_label_embeddings_mod = sbert_model.encode(candidate_labels_mod)\n",
    "candidate_label_embeddings_mod = torch.tensor(candidate_label_embeddings_mod)\n",
    "candidate_label_embeddings_mod_transformed = torch.mm(candidate_label_embeddings_mod, transfer_matrix)\n",
    "print(candidate_label_embeddings_mod_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d07098f-3ab3-4a23-a6d7-cc9b8a7ef012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models for NLI approach\n",
    "zero_shot_nli_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abaa3fc-a1fa-426c-b7a6-7a8361d3d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models for NLI cross-encoder approach\n",
    "zero_shot_nli_cross_en_classifier = pipeline(\"zero-shot-classification\", model='cross-encoder/nli-distilroberta-base', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa95d87c-4e3b-4b9f-b04a-4c3b602a4177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and fit multi-label binarizer (one-hot encoding for multi-label)\n",
    "mlb  = MultiLabelBinarizer()\n",
    "mlb.fit([candidate_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901c360a-ba1c-426a-9a73-5a6db1051fd0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Get classifications with BartLargeMNLI, CrossEncoderNLI, and SBERT (to avoid redundant computations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156941cb-6faa-413a-9f57-430a40aded46",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get classification with BartLargeMNLI - test case name\n",
    "analyzed_key = []\n",
    "classif_res_list_BartLargeMNLI_name = []\n",
    "\n",
    "for index,row in test_case_name_df.iterrows():\n",
    "    # Get test case name\n",
    "    test_key = row['id']\n",
    "    test_description = row['description']\n",
    "    correct_labels = row['labels']\n",
    "\n",
    "    if test_key in analyzed_key:\n",
    "        continue\n",
    "\n",
    "    # Embed test description\n",
    "    classification_res = zero_shot_nli_classifier(test_description, candidate_labels, multi_label=True)\n",
    "    classif_res_list_BartLargeMNLI_name.append(classification_res)\n",
    "    analyzed_key.append(test_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0efcc8-49c1-4164-8c8c-50df151a5be6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get classification with BartLargeMNLI - test case name + objective\n",
    "analyzed_key = []\n",
    "classif_res_list_BartLargeMNLI_name_obj = []\n",
    "\n",
    "for index,row in test_case_name_obj_df.iterrows():\n",
    "    # Get test case name\n",
    "    test_key = row['id']\n",
    "    test_description = row['description']\n",
    "    correct_labels = row['labels']\n",
    "\n",
    "    if test_key in analyzed_key:\n",
    "        continue\n",
    "\n",
    "    # Embed test description\n",
    "    classification_res = zero_shot_nli_classifier(test_description, candidate_labels, multi_label=True)\n",
    "    classif_res_list_BartLargeMNLI_name_obj.append(classification_res)\n",
    "    analyzed_key.append(test_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7775eec0-6118-4de4-997b-7ee1226235f3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get classification with CrossEncoderNLI - test case name\n",
    "analyzed_key = []\n",
    "classif_res_list_CrossEncoderNLI_name = []\n",
    "\n",
    "for index,row in test_case_name_df.iterrows():\n",
    "    # Get test case name\n",
    "    test_key = row['id']\n",
    "    test_description = row['description']\n",
    "    correct_labels = row['labels']\n",
    "\n",
    "    if test_key in analyzed_key:\n",
    "        continue\n",
    "\n",
    "    # Embed test description\n",
    "    classification_res = zero_shot_nli_cross_en_classifier(test_description, candidate_labels, multi_label=True)\n",
    "    classif_res_list_CrossEncoderNLI_name.append(classification_res)\n",
    "    analyzed_key.append(test_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a520211f-b593-4348-894b-2e6b8a94ee07",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get classification with CrossEncoderNLI - test case name + objective\n",
    "analyzed_key = []\n",
    "classif_res_list_CrossEncoderNLI_name_obj = []\n",
    "\n",
    "for index,row in test_case_name_obj_df.iterrows():\n",
    "    # Get test case name\n",
    "    test_key = row['id']\n",
    "    test_description = row['description']\n",
    "    correct_labels = row['labels']\n",
    "\n",
    "    if test_key in analyzed_key:\n",
    "        continue\n",
    "\n",
    "    # Embed test description\n",
    "    classification_res = zero_shot_nli_cross_en_classifier(test_description, candidate_labels, multi_label=True)\n",
    "    classif_res_list_CrossEncoderNLI_name_obj.append(classification_res)\n",
    "    analyzed_key.append(test_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eaa6f1-b26c-4138-9dd5-0ce1d4f9e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get classification from SBERT - test case name\n",
    "analyzed_key = []\n",
    "classif_res_list_SBERT_name = []\n",
    "\n",
    "for index,row in test_case_name_df.iterrows():\n",
    "    # Add test case name\n",
    "    test_key = row['id']\n",
    "    test_description = row['description']\n",
    "\n",
    "    if test_key in analyzed_key:\n",
    "        continue\n",
    "\n",
    "    # Embed test description\n",
    "    sentence_embedding = sbert_model.encode(test_description)\n",
    "\n",
    "    # Apply linear transformation to sentence embedding\n",
    "    sentence_embedding = torch.tensor(sentence_embedding)\n",
    "    sentence_embedding = torch.reshape(sentence_embedding, (1, len(sentence_embedding)))\n",
    "    sentence_embedding_transformed = torch.mm(sentence_embedding, transfer_matrix)\n",
    "                \n",
    "    classif_res_list_SBERT_name.append(sentence_embedding_transformed)\n",
    "    analyzed_key.append(test_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc151c-4219-4c75-a729-d73ac18d4932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get classification from SBERT - test case name + objective\n",
    "analyzed_key = []\n",
    "classif_res_list_SBERT_name_obj = []\n",
    "\n",
    "for index,row in test_case_name_obj_df.iterrows():\n",
    "    # Add test case name\n",
    "    test_key = row['id']\n",
    "    test_description = row['description']\n",
    "\n",
    "    if test_key in analyzed_key:\n",
    "        continue\n",
    "\n",
    "    # Embed test description\n",
    "    sentence_embedding = sbert_model.encode(test_description)\n",
    "\n",
    "    # Apply linear transformation to sentence embedding\n",
    "    sentence_embedding = torch.tensor(sentence_embedding)\n",
    "    sentence_embedding = torch.reshape(sentence_embedding, (1, len(sentence_embedding)))\n",
    "    sentence_embedding_transformed = torch.mm(sentence_embedding, transfer_matrix)\n",
    "                \n",
    "    classif_res_list_SBERT_name_obj.append(sentence_embedding_transformed)\n",
    "    analyzed_key.append(test_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c88cd7f-fd5d-47bb-a47c-4fbc7ec336e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### EnsMajorVoting - Ensemble with majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ae21c0-f11c-4ad2-81f2-407ebc167c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define name and description of experiment\n",
    "experiment_name = \"Ensemble with majority voting - Test case name and objective\"\n",
    "experiment_active = mlflow.set_experiment(experiment_name)\n",
    "experiment_id = experiment_active.experiment_id\n",
    "MlflowClient().set_experiment_tag(experiment_id, \n",
    "     \"mlflow.note.content\",\"Evaluate how the ensemble of all the zero-shot techniques using majority voting performs for test case name and objective.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09dfe61-c57e-46dd-9a2b-8970f27708b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(experiment_dir)\n",
    "experiment_active = mlflow.set_experiment(experiment_name)\n",
    "run_name = \"Test case name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a696c7-44df-41ab-9c68-c522f531555c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use optimal similarity thresholds as found in the experiments with individual models\n",
    "optimal_sim_thresh_embedding = 0.2\n",
    "optimal_sim_thresh_nli = 0.9\n",
    "optimal_sim_thresh_nli_cross_enc = 0.6\n",
    "\n",
    "# Dataframe to store embedding predictions\n",
    "classified_test_cases_embedding = pd.DataFrame(columns=['test_key', 'test_name', 'labels'])\n",
    "\n",
    "# Dataframe to store NLI predictions\n",
    "classified_test_cases_nli = pd.DataFrame(columns=['test_key', 'test_name', 'labels'])\n",
    "\n",
    "# Dataframe to store cross-encoder NLI predictions\n",
    "classified_test_cases_nli_cross_enc = pd.DataFrame(columns=['test_key', 'test_name', 'labels'])\n",
    "\n",
    "index_add = 0\n",
    "analyzed_key = []\n",
    "\n",
    "for index in range(len(test_case_name_df)):\n",
    "    row = test_case_name_df.iloc[index,:]     \n",
    "    test_key = row['id']\n",
    "    test_description = row['description']\n",
    "    correct_labels = row['labels']\n",
    "\n",
    "    if test_key in analyzed_key:\n",
    "        continue\n",
    "\n",
    "    # Get predictions with latent embeddings\n",
    "    sentence_embedding_transformed = classif_res_list_SBERT_name[index]\n",
    "\n",
    "    labels_to_include = set()\n",
    "    \n",
    "    # Iterate through candidate labels and the modified candidate label embeddings\n",
    "    for label, label_embedding in zip(candidate_labels, candidate_label_embeddings_mod_transformed):\n",
    "        cos_sim = util.cos_sim(sentence_embedding_transformed, label_embedding)\n",
    "        if cos_sim > optimal_sim_thresh_embedding:\n",
    "            labels_to_include.add(label)\n",
    "\n",
    "    classified_test_cases_embedding.loc[index_add] = [test_key, test_description, labels_to_include]\n",
    "\n",
    "\n",
    "    # Get predictions from NLI model\n",
    "    classif_res = classif_res_list_BartLargeMNLI_name[index]\n",
    "\n",
    "    # Check we are looking at the correct instance\n",
    "    if test_description != classif_res['sequence']:\n",
    "        print(\"Error!\")\n",
    "        break\n",
    "\n",
    "    predicted_labels = classif_res['labels']\n",
    "    prediction_scores = classif_res['scores']\n",
    "\n",
    "    # Iterate through predictions (descendent order by score)\n",
    "    labels_to_include = set()\n",
    "    scores_to_include = []\n",
    "    for label,score in zip(predicted_labels, prediction_scores):\n",
    "        if score < optimal_sim_thresh_nli:\n",
    "            break\n",
    "        else:\n",
    "            labels_to_include.add(label)\n",
    "            scores_to_include.append(score) \n",
    "\n",
    "    classified_test_cases_nli.loc[index_add] = [test_key, test_description, labels_to_include]\n",
    "\n",
    "\n",
    "    # Get predictions from cross-encoder NLI model\n",
    "    classif_res = classif_res_list_CrossEncoderNLI_name[index]\n",
    "\n",
    "    # Check we are looking at the correct instance\n",
    "    if test_description != classif_res['sequence']:\n",
    "        print(\"Error!\")\n",
    "        break\n",
    "\n",
    "    predicted_labels = classif_res['labels']\n",
    "    prediction_scores = classif_res['scores']\n",
    "\n",
    "    # Iterate through predictions (descendent order by score)\n",
    "    labels_to_include = set()\n",
    "    scores_to_include = []\n",
    "    for label,score in zip(predicted_labels, prediction_scores):\n",
    "        if score < optimal_sim_thresh_nli_cross_enc:\n",
    "            break\n",
    "        else:\n",
    "            labels_to_include.add(label)\n",
    "            scores_to_include.append(score) \n",
    "\n",
    "    classified_test_cases_nli_cross_enc.loc[index_add] = [test_key, test_description, labels_to_include]\n",
    "    index_add += 1\n",
    "    analyzed_key.append(test_key)\n",
    "\n",
    "# Dataframe to store combination of predictions\n",
    "final_classified_test_cases = pd.DataFrame(columns=['test_key', 'test_name', 'correct_labels', 'pred_labels', 'encoded_correct_labels', 'encoded_pred_labels'])\n",
    "index_add = 0\n",
    "\n",
    "analyzed_key = []\n",
    "\n",
    "for index in range(len(classified_test_cases_nli)):\n",
    "    row = classified_test_cases_nli.iloc[index,:]\n",
    "    test_key_nli = row['test_key']\n",
    "    test_name_nli = row['test_name']\n",
    "    labels_nli = row['labels']\n",
    "\n",
    "    if test_key_nli in analyzed_key:\n",
    "        continue\n",
    "\n",
    "    row = classified_test_cases_nli_cross_enc.iloc[index,:]\n",
    "    test_key_nli_cross_enc = row['test_key']\n",
    "    test_name_nli_cross_enc = row['test_name']\n",
    "    labels_nli_cross_enc = row['labels']\n",
    "\n",
    "\n",
    "    row_emb = classified_test_cases_embedding.iloc[index,:]\n",
    "    test_key_emb = row_emb['test_key']\n",
    "    test_name_emb = row_emb['test_name']\n",
    "    labels_emb = row_emb['labels']\n",
    "\n",
    "    if (test_key_nli != test_key_nli_cross_enc) or (test_key_nli != test_key_emb):\n",
    "        print(\"Error occurred!\")\n",
    "        break\n",
    "\n",
    "    # Count number of models that assigned each label to current test case\n",
    "    count_label = { i : 0 for i in candidate_labels }\n",
    "    for label in candidate_labels:\n",
    "        if label in labels_nli:\n",
    "            count_label[label] += count_label[label] + 1\n",
    "\n",
    "        if label in labels_nli_cross_enc:\n",
    "            count_label[label] += count_label[label] + 1\n",
    "\n",
    "        if label in labels_emb:\n",
    "            count_label[label] += count_label[label] + 1\n",
    "\n",
    "    # Select labels assigned by at least 2 models\n",
    "    intersection_labels = set()\n",
    "    for label in count_label:\n",
    "        if count_label[label] >= 2:\n",
    "            intersection_labels.add(label)\n",
    "\n",
    "    correct_labels = test_case_name_df.iloc[index,3]\n",
    "\n",
    "    # Encoded labels\n",
    "    encoded_correct_labels = mlb.transform([correct_labels])\n",
    "    encoded_labels_to_include = mlb.transform([intersection_labels])     \n",
    "\n",
    "    final_classified_test_cases.loc[index_add] = [test_key_nli, test_name_nli, correct_labels, intersection_labels, encoded_correct_labels, encoded_labels_to_include]\n",
    "    index_add += 1\n",
    "\n",
    "    analyzed_key.append(test_key_nli)\n",
    "\n",
    "\n",
    "with mlflow.start_run(experiment_id=experiment_active.experiment_id, run_name=run_name):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for index,row in final_classified_test_cases.iterrows():\n",
    "        label_encoded = row['encoded_correct_labels']\n",
    "        predicted_label_encoded = row['encoded_pred_labels']\n",
    "        y_true.append(label_encoded[0])\n",
    "        y_pred.append(predicted_label_encoded[0])\n",
    "\n",
    "    # Get metrics\n",
    "    metrics = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    precision = metrics[0]\n",
    "    recall = metrics[1]\n",
    "    fscore = metrics[2]\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    duration = \"{0:.2f}\".format(duration)\n",
    "\n",
    "    mlflow.log_param(\"Embedding similarity threshold\", optimal_sim_thresh_embedding)\n",
    "    mlflow.log_param(\"NLI similarity threshold\", optimal_sim_thresh_nli)\n",
    "    mlflow.log_param(\"NLI cross-encoder similarity threshold\", optimal_sim_thresh_nli_cross_enc)\n",
    "    mlflow.log_param(\"Execution time\", duration)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"fscore\", fscore) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546326c5-ac3e-405f-9dca-7c4646716ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(experiment_dir)\n",
    "experiment_active = mlflow.set_experiment(experiment_name)\n",
    "run_name = \"Test case name + objective\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd14d9e-7112-4bae-b6fb-e89f96b3e57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use optimal similarity thresholds as found in the experiments with individual models\n",
    "optimal_sim_thresh_embedding = 0.2\n",
    "optimal_sim_thresh_nli = 0.9\n",
    "optimal_sim_thresh_nli_cross_enc = 0.6\n",
    "\n",
    "# Dataframe to store embedding predictions\n",
    "classified_test_cases_embedding = pd.DataFrame(columns=['test_key', 'test_name', 'labels'])\n",
    "\n",
    "# Dataframe to store NLI predictions\n",
    "classified_test_cases_nli = pd.DataFrame(columns=['test_key', 'test_name', 'labels'])\n",
    "\n",
    "# Dataframe to store cross-encoder NLI predictions\n",
    "classified_test_cases_nli_cross_enc = pd.DataFrame(columns=['test_key', 'test_name', 'labels'])\n",
    "\n",
    "index_add = 0\n",
    "analyzed_key = []\n",
    "\n",
    "for index in range(len(test_case_name_obj_df)):\n",
    "    row = test_case_name_df.iloc[index,:]     \n",
    "    test_key = row['id']\n",
    "    test_description = row['description']\n",
    "    correct_labels = row['labels']\n",
    "\n",
    "    if test_key in analyzed_key:\n",
    "        continue\n",
    "\n",
    "    # Get predictions with latent embeddings\n",
    "    sentence_embedding_transformed = classif_res_list_SBERT_name_obj[index]\n",
    "\n",
    "    labels_to_include = set()\n",
    "    \n",
    "    # Iterate through candidate labels and the modified candidate label embeddings\n",
    "    for label, label_embedding in zip(candidate_labels, candidate_label_embeddings_mod_transformed):\n",
    "        cos_sim = util.cos_sim(sentence_embedding_transformed, label_embedding)\n",
    "        if cos_sim > optimal_sim_thresh_embedding:\n",
    "            labels_to_include.add(label)\n",
    "\n",
    "    classified_test_cases_embedding.loc[index_add] = [test_key, test_description, labels_to_include]\n",
    "\n",
    "    \n",
    "    # Get predictions from NLI model\n",
    "    classif_res = classif_res_list_BartLargeMNLI_name_obj[index]\n",
    "\n",
    "    # Check we are looking at the correct instance\n",
    "    if test_description != classif_res['sequence']:\n",
    "        print(\"Error!\")\n",
    "        break\n",
    "\n",
    "    predicted_labels = classif_res['labels']\n",
    "    prediction_scores = classif_res['scores']\n",
    "\n",
    "    # Iterate through predictions (descendent order by score)\n",
    "    labels_to_include = set()\n",
    "    scores_to_include = []\n",
    "    for label,score in zip(predicted_labels, prediction_scores):\n",
    "        if score < optimal_sim_thresh_nli:\n",
    "            break\n",
    "        else:\n",
    "            labels_to_include.add(label)\n",
    "            scores_to_include.append(score) \n",
    "\n",
    "    classified_test_cases_nli.loc[index_add] = [test_key, test_description, labels_to_include]\n",
    "\n",
    "\n",
    "    # Get predictions from cross-encoder NLI model\n",
    "    classif_res = classif_res_list_CrossEncoderNLI_name_obj[index]\n",
    "\n",
    "    # Check we are looking at the correct instance\n",
    "    if test_description != classif_res['sequence']:\n",
    "        print(\"Error!\")\n",
    "        break\n",
    "\n",
    "    predicted_labels = classif_res['labels']\n",
    "    prediction_scores = classif_res['scores']\n",
    "\n",
    "    # Iterate through predictions (descendent order by score)\n",
    "    labels_to_include = set()\n",
    "    scores_to_include = []\n",
    "    for label,score in zip(predicted_labels, prediction_scores):\n",
    "        if score < optimal_sim_thresh_nli_cross_enc:\n",
    "            break\n",
    "        else:\n",
    "            labels_to_include.add(label)\n",
    "            scores_to_include.append(score) \n",
    "\n",
    "    classified_test_cases_nli_cross_enc.loc[index_add] = [test_key, test_description, labels_to_include]\n",
    "    index_add += 1\n",
    "    analyzed_key.append(test_key)\n",
    "\n",
    "# Dataframe to store combination of predictions\n",
    "final_classified_test_cases = pd.DataFrame(columns=['test_key', 'test_name', 'correct_labels', 'pred_labels', 'encoded_correct_labels', 'encoded_pred_labels'])\n",
    "index_add = 0\n",
    "\n",
    "analyzed_key = []\n",
    "\n",
    "for index in range(len(classified_test_cases_nli)):\n",
    "    row = classified_test_cases_nli.iloc[index,:]\n",
    "    test_key_nli = row['test_key']\n",
    "    test_name_nli = row['test_name']\n",
    "    labels_nli = row['labels']\n",
    "\n",
    "    if test_key_nli in analyzed_key:\n",
    "        continue\n",
    "\n",
    "    row = classified_test_cases_nli_cross_enc.iloc[index,:]\n",
    "    test_key_nli_cross_enc = row['test_key']\n",
    "    test_name_nli_cross_enc = row['test_name']\n",
    "    labels_nli_cross_enc = row['labels']\n",
    "\n",
    "\n",
    "    row_emb = classified_test_cases_embedding.iloc[index,:]\n",
    "    test_key_emb = row_emb['test_key']\n",
    "    test_name_emb = row_emb['test_name']\n",
    "    labels_emb = row_emb['labels']\n",
    "\n",
    "    if (test_key_nli != test_key_nli_cross_enc) or (test_key_nli != test_key_emb):\n",
    "        print(\"Error occurred!\")\n",
    "        break\n",
    "\n",
    "    # Count number of models that assigned each label to current test case\n",
    "    count_label = { i : 0 for i in candidate_labels }\n",
    "    for label in candidate_labels:\n",
    "        if label in labels_nli:\n",
    "            count_label[label] += count_label[label] + 1\n",
    "\n",
    "        if label in labels_nli_cross_enc:\n",
    "            count_label[label] += count_label[label] + 1\n",
    "\n",
    "        if label in labels_emb:\n",
    "            count_label[label] += count_label[label] + 1\n",
    "\n",
    "    # Select labels assigned by at least 2 models\n",
    "    intersection_labels = set()\n",
    "    for label in count_label:\n",
    "        if count_label[label] >= 2:\n",
    "            intersection_labels.add(label)\n",
    "\n",
    "    correct_labels = test_case_name_obj_df.iloc[index,3]\n",
    "\n",
    "    # Encoded labels\n",
    "    encoded_correct_labels = mlb.transform([correct_labels])\n",
    "    encoded_labels_to_include = mlb.transform([intersection_labels])     \n",
    "\n",
    "    final_classified_test_cases.loc[index_add] = [test_key_nli, test_name_nli, correct_labels, intersection_labels, encoded_correct_labels, encoded_labels_to_include]\n",
    "    index_add += 1\n",
    "\n",
    "    analyzed_key.append(test_key_nli)\n",
    "\n",
    "\n",
    "with mlflow.start_run(experiment_id=experiment_active.experiment_id, run_name=run_name):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for index,row in final_classified_test_cases.iterrows():\n",
    "        label_encoded = row['encoded_correct_labels']\n",
    "        predicted_label_encoded = row['encoded_pred_labels']\n",
    "        y_true.append(label_encoded[0])\n",
    "        y_pred.append(predicted_label_encoded[0])\n",
    "\n",
    "    # Get metrics\n",
    "    metrics = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    precision = metrics[0]\n",
    "    recall = metrics[1]\n",
    "    fscore = metrics[2]\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    duration = \"{0:.2f}\".format(duration)\n",
    "\n",
    "    mlflow.log_param(\"Embedding similarity threshold\", optimal_sim_thresh_embedding)\n",
    "    mlflow.log_param(\"NLI similarity threshold\", optimal_sim_thresh_nli)\n",
    "    mlflow.log_param(\"NLI cross-encoder similarity threshold\", optimal_sim_thresh_nli_cross_enc)\n",
    "    mlflow.log_param(\"Execution time\", duration)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"fscore\", fscore) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98f97f6-9f84-4749-9e91-4c6eeb1f2af0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### EnsFullInters - Ensemble with full intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252a7606-732d-45f6-b2e8-63e8a857f825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define name and description of experiment\n",
    "experiment_name = \"Ensemble with full intersection - Test case name and objective\"\n",
    "experiment_active = mlflow.set_experiment(experiment_name)\n",
    "experiment_id = experiment_active.experiment_id\n",
    "MlflowClient().set_experiment_tag(experiment_id, \n",
    "     \"mlflow.note.content\",\"Evaluate how the ensemble of full intersection of all the zero-shot techniques performs for test case name and objective.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a002b9-92e0-45da-958a-ba91a5a397e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(experiment_dir)\n",
    "experiment_active = mlflow.set_experiment(experiment_name)\n",
    "run_name = \"Test case name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f19d1f-515e-47b0-afbd-0f162fe0c0c2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use optimal similarity thresholds as found in the experiments with individual models\n",
    "optimal_sim_thresh_embedding = 0.2\n",
    "optimal_sim_thresh_nli = 0.9\n",
    "optimal_sim_thresh_nli_cross_enc = 0.6\n",
    "\n",
    "# Dataframe to store embedding predictions\n",
    "classified_test_cases_embedding = pd.DataFrame(columns=['test_key', 'test_name', 'labels'])\n",
    "\n",
    "# Dataframe to store NLI predictions\n",
    "classified_test_cases_nli = pd.DataFrame(columns=['test_key', 'test_name', 'labels'])\n",
    "\n",
    "# Dataframe to store cross-encoder NLI predictions\n",
    "classified_test_cases_nli_cross_enc = pd.DataFrame(columns=['test_key', 'test_name', 'labels'])\n",
    "\n",
    "index_add = 0\n",
    "analyzed_key = []\n",
    "\n",
    "for index in range(len(test_case_name_df)):\n",
    "    row = test_case_name_df.iloc[index,:]     \n",
    "    test_key = row['id']\n",
    "    test_description = row['description']\n",
    "    correct_labels = row['labels']\n",
    "\n",
    "    if test_key in analyzed_key:\n",
    "        continue\n",
    "\n",
    "    # Get predictions with latent embeddings\n",
    "    sentence_embedding_transformed = classif_res_list_SBERT_name[index]\n",
    "\n",
    "    labels_to_include = set()\n",
    "    \n",
    "    # Iterate through candidate labels and the modified candidate label embeddings\n",
    "    for label, label_embedding in zip(candidate_labels, candidate_label_embeddings_mod_transformed):\n",
    "        cos_sim = util.cos_sim(sentence_embedding_transformed, label_embedding)\n",
    "        if cos_sim > optimal_sim_thresh_embedding:\n",
    "            labels_to_include.add(label)\n",
    "\n",
    "    classified_test_cases_embedding.loc[index_add] = [test_key, test_description, labels_to_include]\n",
    "\n",
    "\n",
    "    # Get predictions from NLI model\n",
    "    classif_res = classif_res_list_BartLargeMNLI_name[index]\n",
    "\n",
    "    # Check we are looking at the correct instance\n",
    "    if test_description != classif_res['sequence']:\n",
    "        print(\"Error!\")\n",
    "        break\n",
    "\n",
    "    predicted_labels = classif_res['labels']\n",
    "    prediction_scores = classif_res['scores']\n",
    "\n",
    "    # Iterate through predictions (descendent order by score)\n",
    "    labels_to_include = set()\n",
    "    scores_to_include = []\n",
    "    for label,score in zip(predicted_labels, prediction_scores):\n",
    "        if score < optimal_sim_thresh_nli:\n",
    "            break\n",
    "        else:\n",
    "            labels_to_include.add(label)\n",
    "            scores_to_include.append(score) \n",
    "\n",
    "    classified_test_cases_nli.loc[index_add] = [test_key, test_description, labels_to_include]\n",
    "\n",
    "\n",
    "    # Get predictions from cross-encoder NLI model\n",
    "    classif_res = classif_res_list_CrossEncoderNLI_name[index]\n",
    "\n",
    "    # Check we are looking at the correct instance\n",
    "    if test_description != classif_res['sequence']:\n",
    "        print(\"Error!\")\n",
    "        break\n",
    "\n",
    "    predicted_labels = classif_res['labels']\n",
    "    prediction_scores = classif_res['scores']\n",
    "\n",
    "    # Iterate through predictions (descendent order by score)\n",
    "    labels_to_include = set()\n",
    "    scores_to_include = []\n",
    "    for label,score in zip(predicted_labels, prediction_scores):\n",
    "        if score < optimal_sim_thresh_nli_cross_enc:\n",
    "            break\n",
    "        else:\n",
    "            labels_to_include.add(label)\n",
    "            scores_to_include.append(score) \n",
    "\n",
    "    classified_test_cases_nli_cross_enc.loc[index_add] = [test_key, test_description, labels_to_include]\n",
    "    index_add += 1\n",
    "    analyzed_key.append(test_key)\n",
    "\n",
    "# Dataframe to store combination of predictions\n",
    "final_classified_test_cases = pd.DataFrame(columns=['test_key', 'test_name', 'correct_labels', 'pred_labels', 'encoded_correct_labels', 'encoded_pred_labels'])\n",
    "index_add = 0\n",
    "\n",
    "analyzed_key = []\n",
    "\n",
    "for index in range(len(classified_test_cases_nli)):\n",
    "    row = classified_test_cases_nli.iloc[index,:]\n",
    "    test_key_nli = row['test_key']\n",
    "    test_name_nli = row['test_name']\n",
    "    labels_nli = row['labels']\n",
    "\n",
    "    if test_key_nli in analyzed_key:\n",
    "        continue\n",
    "\n",
    "    row = classified_test_cases_nli_cross_enc.iloc[index,:]\n",
    "    test_key_nli_cross_enc = row['test_key']\n",
    "    test_name_nli_cross_enc = row['test_name']\n",
    "    labels_nli_cross_enc = row['labels']\n",
    "\n",
    "\n",
    "    row_emb = classified_test_cases_embedding.iloc[index,:]\n",
    "    test_key_emb = row_emb['test_key']\n",
    "    test_name_emb = row_emb['test_name']\n",
    "    labels_emb = row_emb['labels']\n",
    "\n",
    "    if (test_key_nli != test_key_nli_cross_enc) or (test_key_nli != test_key_emb):\n",
    "        print(\"Error occurred!\")\n",
    "        break\n",
    "\n",
    "    intersection_labels_nli = labels_nli.intersection(labels_nli_cross_enc)\n",
    "    intersection_labels = intersection_labels_nli.intersection(labels_emb)\n",
    "\n",
    "    correct_labels = test_case_name_df.iloc[index,3]\n",
    "\n",
    "    # Encoded labels\n",
    "    encoded_correct_labels = mlb.transform([correct_labels])\n",
    "    encoded_labels_to_include = mlb.transform([intersection_labels])     \n",
    "\n",
    "    final_classified_test_cases.loc[index_add] = [test_key_nli, test_name_nli, correct_labels, intersection_labels, encoded_correct_labels, encoded_labels_to_include]\n",
    "    index_add += 1\n",
    "\n",
    "    analyzed_key.append(test_key_nli)\n",
    "\n",
    "\n",
    "with mlflow.start_run(experiment_id=experiment_active.experiment_id, run_name=run_name):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for index,row in final_classified_test_cases.iterrows():\n",
    "        label_encoded = row['encoded_correct_labels']\n",
    "        predicted_label_encoded = row['encoded_pred_labels']\n",
    "        y_true.append(label_encoded[0])\n",
    "        y_pred.append(predicted_label_encoded[0])\n",
    "\n",
    "    # Get metrics\n",
    "    metrics = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    precision = metrics[0]\n",
    "    recall = metrics[1]\n",
    "    fscore = metrics[2]\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    duration = \"{0:.2f}\".format(duration)\n",
    "\n",
    "    mlflow.log_param(\"Embedding similarity threshold\", optimal_sim_thresh_embedding)\n",
    "    mlflow.log_param(\"NLI similarity threshold\", optimal_sim_thresh_nli)\n",
    "    mlflow.log_param(\"NLI cross-encoder similarity threshold\", optimal_sim_thresh_nli_cross_enc)\n",
    "    mlflow.log_param(\"Execution time\", duration)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"fscore\", fscore) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2db4e4-1671-4d71-b0d1-a6fe4ecca79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(experiment_dir)\n",
    "experiment_active = mlflow.set_experiment(experiment_name)\n",
    "run_name = \"Test case name + objective\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a021188-7084-434f-95e5-8cda3621a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use optimal similarity thresholds as found in the experiments with individual models\n",
    "optimal_sim_thresh_embedding = 0.2\n",
    "optimal_sim_thresh_nli = 0.9\n",
    "optimal_sim_thresh_nli_cross_enc = 0.6\n",
    "\n",
    "# Dataframe to store embedding predictions\n",
    "classified_test_cases_embedding = pd.DataFrame(columns=['test_key', 'test_name', 'labels'])\n",
    "\n",
    "# Dataframe to store NLI predictions\n",
    "classified_test_cases_nli = pd.DataFrame(columns=['test_key', 'test_name', 'labels'])\n",
    "\n",
    "# Dataframe to store cross-encoder NLI predictions\n",
    "classified_test_cases_nli_cross_enc = pd.DataFrame(columns=['test_key', 'test_name', 'labels'])\n",
    "\n",
    "index_add = 0\n",
    "analyzed_key = []\n",
    "\n",
    "for index in range(len(test_case_name_obj_df)):\n",
    "    row = test_case_name_df.iloc[index,:]     \n",
    "    test_key = row['id']\n",
    "    test_description = row['description']\n",
    "    correct_labels = row['labels']\n",
    "\n",
    "    if test_key in analyzed_key:\n",
    "        continue\n",
    "\n",
    "    # Get predictions with latent embeddings\n",
    "    sentence_embedding_transformed = classif_res_list_SBERT_name_obj[index]\n",
    "\n",
    "    labels_to_include = set()\n",
    "    \n",
    "    # Iterate through candidate labels and the modified candidate label embeddings\n",
    "    for label, label_embedding in zip(candidate_labels, candidate_label_embeddings_mod_transformed):\n",
    "        cos_sim = util.cos_sim(sentence_embedding_transformed, label_embedding)\n",
    "        if cos_sim > optimal_sim_thresh_embedding:\n",
    "            labels_to_include.add(label)\n",
    "\n",
    "    classified_test_cases_embedding.loc[index_add] = [test_key, test_description, labels_to_include]\n",
    "\n",
    "    \n",
    "    # Get predictions from NLI model\n",
    "    classif_res = classif_res_list_BartLargeMNLI_name_obj[index]\n",
    "\n",
    "    # Check we are looking at the correct instance\n",
    "    if test_description != classif_res['sequence']:\n",
    "        print(\"Error!\")\n",
    "        break\n",
    "\n",
    "    predicted_labels = classif_res['labels']\n",
    "    prediction_scores = classif_res['scores']\n",
    "\n",
    "    # Iterate through predictions (descendent order by score)\n",
    "    labels_to_include = set()\n",
    "    scores_to_include = []\n",
    "    for label,score in zip(predicted_labels, prediction_scores):\n",
    "        if score < optimal_sim_thresh_nli:\n",
    "            break\n",
    "        else:\n",
    "            labels_to_include.add(label)\n",
    "            scores_to_include.append(score) \n",
    "\n",
    "    classified_test_cases_nli.loc[index_add] = [test_key, test_description, labels_to_include]\n",
    "\n",
    "\n",
    "    # Get predictions from cross-encoder NLI model\n",
    "    classif_res = classif_res_list_CrossEncoderNLI_name_obj[index]\n",
    "\n",
    "    # Check we are looking at the correct instance\n",
    "    if test_description != classif_res['sequence']:\n",
    "        print(\"Error!\")\n",
    "        break\n",
    "\n",
    "    predicted_labels = classif_res['labels']\n",
    "    prediction_scores = classif_res['scores']\n",
    "\n",
    "    # Iterate through predictions (descendent order by score)\n",
    "    labels_to_include = set()\n",
    "    scores_to_include = []\n",
    "    for label,score in zip(predicted_labels, prediction_scores):\n",
    "        if score < optimal_sim_thresh_nli_cross_enc:\n",
    "            break\n",
    "        else:\n",
    "            labels_to_include.add(label)\n",
    "            scores_to_include.append(score) \n",
    "\n",
    "    classified_test_cases_nli_cross_enc.loc[index_add] = [test_key, test_description, labels_to_include]\n",
    "    index_add += 1\n",
    "    analyzed_key.append(test_key)\n",
    "\n",
    "# Dataframe to store combination of predictions\n",
    "final_classified_test_cases = pd.DataFrame(columns=['test_key', 'test_name', 'correct_labels', 'pred_labels', 'encoded_correct_labels', 'encoded_pred_labels'])\n",
    "index_add = 0\n",
    "\n",
    "analyzed_key = []\n",
    "\n",
    "for index in range(len(classified_test_cases_nli)):\n",
    "    row = classified_test_cases_nli.iloc[index,:]\n",
    "    test_key_nli = row['test_key']\n",
    "    test_name_nli = row['test_name']\n",
    "    labels_nli = row['labels']\n",
    "\n",
    "    if test_key_nli in analyzed_key:\n",
    "        continue\n",
    "\n",
    "    row = classified_test_cases_nli_cross_enc.iloc[index,:]\n",
    "    test_key_nli_cross_enc = row['test_key']\n",
    "    test_name_nli_cross_enc = row['test_name']\n",
    "    labels_nli_cross_enc = row['labels']\n",
    "\n",
    "\n",
    "    row_emb = classified_test_cases_embedding.iloc[index,:]\n",
    "    test_key_emb = row_emb['test_key']\n",
    "    test_name_emb = row_emb['test_name']\n",
    "    labels_emb = row_emb['labels']\n",
    "\n",
    "    if (test_key_nli != test_key_nli_cross_enc) or (test_key_nli != test_key_emb):\n",
    "        print(\"Error occurred!\")\n",
    "        break\n",
    "\n",
    "    intersection_labels_nli = labels_nli.intersection(labels_nli_cross_enc)\n",
    "    intersection_labels = intersection_labels_nli.intersection(labels_emb)\n",
    "\n",
    "    correct_labels = test_case_name_obj_df.iloc[index,3]\n",
    "\n",
    "    # Encoded labels\n",
    "    encoded_correct_labels = mlb.transform([correct_labels])\n",
    "    encoded_labels_to_include = mlb.transform([intersection_labels])     \n",
    "\n",
    "    final_classified_test_cases.loc[index_add] = [test_key_nli, test_name_nli, correct_labels, intersection_labels, encoded_correct_labels, encoded_labels_to_include]\n",
    "    index_add += 1\n",
    "\n",
    "    analyzed_key.append(test_key_nli)\n",
    "\n",
    "\n",
    "with mlflow.start_run(experiment_id=experiment_active.experiment_id, run_name=run_name):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for index,row in final_classified_test_cases.iterrows():\n",
    "        label_encoded = row['encoded_correct_labels']\n",
    "        predicted_label_encoded = row['encoded_pred_labels']\n",
    "        y_true.append(label_encoded[0])\n",
    "        y_pred.append(predicted_label_encoded[0])\n",
    "\n",
    "    # Get metrics\n",
    "    metrics = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    precision = metrics[0]\n",
    "    recall = metrics[1]\n",
    "    fscore = metrics[2]\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    duration = \"{0:.2f}\".format(duration)\n",
    "\n",
    "    mlflow.log_param(\"Embedding similarity threshold\", optimal_sim_thresh_embedding)\n",
    "    mlflow.log_param(\"NLI similarity threshold\", optimal_sim_thresh_nli)\n",
    "    mlflow.log_param(\"NLI cross-encoder similarity threshold\", optimal_sim_thresh_nli_cross_enc)\n",
    "    mlflow.log_param(\"Execution time\", duration)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"fscore\", fscore) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc32164-351c-43a0-9ec4-3e28e5ca4dc2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### EnsBackOffTwo - Ensemble with back-off using top-2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115769ef-9d95-4701-a1b1-1cbaeddf87c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define name and description of experiment\n",
    "experiment_name = \"Ensemble with back-off using top-2 models - Test case name and objective\"\n",
    "experiment_active = mlflow.set_experiment(experiment_name)\n",
    "experiment_id = experiment_active.experiment_id\n",
    "MlflowClient().set_experiment_tag(experiment_id, \n",
    "     \"mlflow.note.content\",\"Evaluate how the ensemble with back-off using top-2 models performs for test case name and objective.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a08c204-8f4f-49d5-9c10-993bee283591",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(experiment_dir)\n",
    "experiment_active = mlflow.set_experiment(experiment_name)\n",
    "run_name = \"Test case name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f20931-e567-4912-a92b-37559969132f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use optimal similarity thresholds as found in the experiments with individual models\n",
    "optimal_sim_thresh_embedding = 0.2\n",
    "optimal_sim_thresh_nli = 0.9\n",
    "optimal_sim_thresh_nli_cross_enc = 0.6\n",
    "\n",
    "# Dataframe to store embedding predictions\n",
    "classified_test_cases_embedding = pd.DataFrame(columns=['test_key', 'test_name', 'labels'])\n",
    "\n",
    "# Dataframe to store NLI predictions\n",
    "classified_test_cases_nli = pd.DataFrame(columns=['test_key', 'test_name', 'labels'])\n",
    "\n",
    "# Dataframe to store cross-encoder NLI predictions\n",
    "classified_test_cases_nli_cross_enc = pd.DataFrame(columns=['test_key', 'test_name', 'labels'])\n",
    "\n",
    "index_add = 0\n",
    "analyzed_key = []\n",
    "\n",
    "for index in range(len(test_case_name_df)):\n",
    "    row = test_case_name_df.iloc[index,:]     \n",
    "    test_key = row['id']\n",
    "    test_description = row['description']\n",
    "    correct_labels = row['labels']\n",
    "\n",
    "    if test_key in analyzed_key:\n",
    "        continue\n",
    "\n",
    "    # Get predictions with latent embeddings\n",
    "    sentence_embedding_transformed = classif_res_list_SBERT_name[index]\n",
    "\n",
    "    labels_to_include = set()\n",
    "    \n",
    "    # Iterate through candidate labels and the modified candidate label embeddings\n",
    "    for label, label_embedding in zip(candidate_labels, candidate_label_embeddings_mod_transformed):\n",
    "        cos_sim = util.cos_sim(sentence_embedding_transformed, label_embedding)\n",
    "        if cos_sim > optimal_sim_thresh_embedding:\n",
    "            labels_to_include.add(label)\n",
    "\n",
    "    classified_test_cases_embedding.loc[index_add] = [test_key, test_description, labels_to_include]\n",
    "\n",
    "\n",
    "    # Get predictions from NLI model\n",
    "    classif_res = classif_res_list_BartLargeMNLI_name[index]\n",
    "\n",
    "    # Check we are looking at the correct instance\n",
    "    if test_description != classif_res['sequence']:\n",
    "        print(\"Error!\")\n",
    "        break\n",
    "\n",
    "    predicted_labels = classif_res['labels']\n",
    "    prediction_scores = classif_res['scores']\n",
    "\n",
    "    # Iterate through predictions (descendent order by score)\n",
    "    labels_to_include = set()\n",
    "    scores_to_include = []\n",
    "    for label,score in zip(predicted_labels, prediction_scores):\n",
    "        if score < optimal_sim_thresh_nli:\n",
    "            break\n",
    "        else:\n",
    "            labels_to_include.add(label)\n",
    "            scores_to_include.append(score) \n",
    "\n",
    "    classified_test_cases_nli.loc[index_add] = [test_key, test_description, labels_to_include]\n",
    "\n",
    "\n",
    "    # Get predictions from cross-encoder NLI model\n",
    "    classif_res = classif_res_list_CrossEncoderNLI_name[index]\n",
    "\n",
    "    # Check we are looking at the correct instance\n",
    "    if test_description != classif_res['sequence']:\n",
    "        print(\"Error!\")\n",
    "        break\n",
    "\n",
    "    predicted_labels = classif_res['labels']\n",
    "    prediction_scores = classif_res['scores']\n",
    "\n",
    "    # Iterate through predictions (descendent order by score)\n",
    "    labels_to_include = set()\n",
    "    scores_to_include = []\n",
    "    for label,score in zip(predicted_labels, prediction_scores):\n",
    "        if score < optimal_sim_thresh_nli_cross_enc:\n",
    "            break\n",
    "        else:\n",
    "            labels_to_include.add(label)\n",
    "            scores_to_include.append(score) \n",
    "\n",
    "    classified_test_cases_nli_cross_enc.loc[index_add] = [test_key, test_description, labels_to_include]\n",
    "    index_add += 1\n",
    "    analyzed_key.append(test_key)\n",
    "\n",
    "# Dataframe to store combination of predictions\n",
    "final_classified_test_cases = pd.DataFrame(columns=['test_key', 'test_name', 'correct_labels', 'pred_labels', 'encoded_correct_labels', 'encoded_pred_labels'])\n",
    "index_add = 0\n",
    "\n",
    "analyzed_key = []\n",
    "\n",
    "for index in range(len(classified_test_cases_nli)):\n",
    "    row = classified_test_cases_nli.iloc[index,:]\n",
    "    test_key_nli = row['test_key']\n",
    "    test_name_nli = row['test_name']\n",
    "    labels_nli = row['labels']\n",
    "\n",
    "    if test_key_nli in analyzed_key:\n",
    "        continue\n",
    "\n",
    "    row = classified_test_cases_nli_cross_enc.iloc[index,:]\n",
    "    test_key_nli_cross_enc = row['test_key']\n",
    "    test_name_nli_cross_enc = row['test_name']\n",
    "    labels_nli_cross_enc = row['labels']\n",
    "\n",
    "\n",
    "    row_emb = classified_test_cases_embedding.iloc[index,:]\n",
    "    test_key_emb = row_emb['test_key']\n",
    "    test_name_emb = row_emb['test_name']\n",
    "    labels_emb = row_emb['labels']\n",
    "\n",
    "    if (test_key_nli != test_key_nli_cross_enc) or (test_key_nli != test_key_emb):\n",
    "        print(\"Error occurred!\")\n",
    "        break\n",
    "\n",
    "    intersection_labels_nli = labels_nli.intersection(labels_nli_cross_enc)\n",
    "    intersection_labels = intersection_labels_nli.intersection(labels_emb)\n",
    "\n",
    "    if len(intersection_labels) == 0:\n",
    "        intersection_labels = labels_emb.intersection(labels_nli)\n",
    "\n",
    "        if len(intersection_labels) == 0:\n",
    "            intersection_labels = labels_emb.intersection(labels_nli_cross_enc)\n",
    "\n",
    "    correct_labels = test_case_name_df.iloc[index,3]\n",
    "\n",
    "    # Encoded labels\n",
    "    encoded_correct_labels = mlb.transform([correct_labels])\n",
    "    encoded_labels_to_include = mlb.transform([intersection_labels])     \n",
    "\n",
    "    final_classified_test_cases.loc[index_add] = [test_key_nli, test_name_nli, correct_labels, intersection_labels, encoded_correct_labels, encoded_labels_to_include]\n",
    "    index_add += 1\n",
    "\n",
    "    analyzed_key.append(test_key_nli)\n",
    "\n",
    "\n",
    "with mlflow.start_run(experiment_id=experiment_active.experiment_id, run_name=run_name):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for index,row in final_classified_test_cases.iterrows():\n",
    "        label_encoded = row['encoded_correct_labels']\n",
    "        predicted_label_encoded = row['encoded_pred_labels']\n",
    "        y_true.append(label_encoded[0])\n",
    "        y_pred.append(predicted_label_encoded[0])\n",
    "\n",
    "    # Get metrics\n",
    "    metrics = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    precision = metrics[0]\n",
    "    recall = metrics[1]\n",
    "    fscore = metrics[2]\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    duration = \"{0:.2f}\".format(duration)\n",
    "\n",
    "    mlflow.log_param(\"Embedding similarity threshold\", optimal_sim_thresh_embedding)\n",
    "    mlflow.log_param(\"NLI similarity threshold\", optimal_sim_thresh_nli)\n",
    "    mlflow.log_param(\"NLI cross-encoder similarity threshold\", optimal_sim_thresh_nli_cross_enc)\n",
    "    mlflow.log_param(\"Execution time\", duration)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"fscore\", fscore) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6fa439-81b2-4ef9-9daa-af12cee0c515",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(experiment_dir)\n",
    "experiment_active = mlflow.set_experiment(experiment_name)\n",
    "run_name = \"Test case name + objective\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bfef05-7e99-4684-b392-3c2ed286c9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use optimal similarity thresholds as found in the experiments with individual models\n",
    "optimal_sim_thresh_embedding = 0.2\n",
    "optimal_sim_thresh_nli = 0.9\n",
    "optimal_sim_thresh_nli_cross_enc = 0.6\n",
    "\n",
    "# Dataframe to store embedding predictions\n",
    "classified_test_cases_embedding = pd.DataFrame(columns=['test_key', 'test_name', 'labels'])\n",
    "\n",
    "# Dataframe to store NLI predictions\n",
    "classified_test_cases_nli = pd.DataFrame(columns=['test_key', 'test_name', 'labels'])\n",
    "\n",
    "# Dataframe to store cross-encoder NLI predictions\n",
    "classified_test_cases_nli_cross_enc = pd.DataFrame(columns=['test_key', 'test_name', 'labels'])\n",
    "\n",
    "index_add = 0\n",
    "analyzed_key = []\n",
    "\n",
    "for index in range(len(test_case_name_obj_df)):\n",
    "    row = test_case_name_df.iloc[index,:]     \n",
    "    test_key = row['id']\n",
    "    test_description = row['description']\n",
    "    correct_labels = row['labels']\n",
    "\n",
    "    if test_key in analyzed_key:\n",
    "        continue\n",
    "\n",
    "    # Get predictions with latent embeddings\n",
    "    sentence_embedding_transformed = classif_res_list_SBERT_name_obj[index]\n",
    "\n",
    "    labels_to_include = set()\n",
    "    \n",
    "    # Iterate through candidate labels and the modified candidate label embeddings\n",
    "    for label, label_embedding in zip(candidate_labels, candidate_label_embeddings_mod_transformed):\n",
    "        cos_sim = util.cos_sim(sentence_embedding_transformed, label_embedding)\n",
    "        if cos_sim > optimal_sim_thresh_embedding:\n",
    "            labels_to_include.add(label)\n",
    "\n",
    "    classified_test_cases_embedding.loc[index_add] = [test_key, test_description, labels_to_include]\n",
    "\n",
    "    \n",
    "    # Get predictions from NLI model\n",
    "    classif_res = classif_res_list_BartLargeMNLI_name_obj[index]\n",
    "\n",
    "    # Check we are looking at the correct instance\n",
    "    if test_description != classif_res['sequence']:\n",
    "        print(\"Error!\")\n",
    "        break\n",
    "\n",
    "    predicted_labels = classif_res['labels']\n",
    "    prediction_scores = classif_res['scores']\n",
    "\n",
    "    # Iterate through predictions (descendent order by score)\n",
    "    labels_to_include = set()\n",
    "    scores_to_include = []\n",
    "    for label,score in zip(predicted_labels, prediction_scores):\n",
    "        if score < optimal_sim_thresh_nli:\n",
    "            break\n",
    "        else:\n",
    "            labels_to_include.add(label)\n",
    "            scores_to_include.append(score) \n",
    "\n",
    "    classified_test_cases_nli.loc[index_add] = [test_key, test_description, labels_to_include]\n",
    "\n",
    "\n",
    "    # Get predictions from cross-encoder NLI model\n",
    "    classif_res = classif_res_list_CrossEncoderNLI_name_obj[index]\n",
    "\n",
    "    # Check we are looking at the correct instance\n",
    "    if test_description != classif_res['sequence']:\n",
    "        print(\"Error!\")\n",
    "        break\n",
    "\n",
    "    predicted_labels = classif_res['labels']\n",
    "    prediction_scores = classif_res['scores']\n",
    "\n",
    "    # Iterate through predictions (descendent order by score)\n",
    "    labels_to_include = set()\n",
    "    scores_to_include = []\n",
    "    for label,score in zip(predicted_labels, prediction_scores):\n",
    "        if score < optimal_sim_thresh_nli_cross_enc:\n",
    "            break\n",
    "        else:\n",
    "            labels_to_include.add(label)\n",
    "            scores_to_include.append(score) \n",
    "\n",
    "    classified_test_cases_nli_cross_enc.loc[index_add] = [test_key, test_description, labels_to_include]\n",
    "    index_add += 1\n",
    "    analyzed_key.append(test_key)\n",
    "\n",
    "# Dataframe to store combination of predictions\n",
    "final_classified_test_cases = pd.DataFrame(columns=['test_key', 'test_name', 'correct_labels', 'pred_labels', 'encoded_correct_labels', 'encoded_pred_labels'])\n",
    "index_add = 0\n",
    "\n",
    "analyzed_key = []\n",
    "\n",
    "for index in range(len(classified_test_cases_nli)):\n",
    "    row = classified_test_cases_nli.iloc[index,:]\n",
    "    test_key_nli = row['test_key']\n",
    "    test_name_nli = row['test_name']\n",
    "    labels_nli = row['labels']\n",
    "\n",
    "    if test_key_nli in analyzed_key:\n",
    "        continue\n",
    "\n",
    "    row = classified_test_cases_nli_cross_enc.iloc[index,:]\n",
    "    test_key_nli_cross_enc = row['test_key']\n",
    "    test_name_nli_cross_enc = row['test_name']\n",
    "    labels_nli_cross_enc = row['labels']\n",
    "\n",
    "\n",
    "    row_emb = classified_test_cases_embedding.iloc[index,:]\n",
    "    test_key_emb = row_emb['test_key']\n",
    "    test_name_emb = row_emb['test_name']\n",
    "    labels_emb = row_emb['labels']\n",
    "\n",
    "    if (test_key_nli != test_key_nli_cross_enc) or (test_key_nli != test_key_emb):\n",
    "        print(\"Error occurred!\")\n",
    "        break\n",
    "\n",
    "    intersection_labels_nli = labels_nli.intersection(labels_nli_cross_enc)\n",
    "    intersection_labels = intersection_labels_nli.intersection(labels_emb)\n",
    "\n",
    "    if len(intersection_labels) == 0:\n",
    "        intersection_labels = labels_emb.intersection(labels_nli)\n",
    "\n",
    "        if len(intersection_labels) == 0:\n",
    "            intersection_labels = labels_emb.intersection(labels_nli_cross_enc)\n",
    "\n",
    "    correct_labels = test_case_name_obj_df.iloc[index,3]\n",
    "\n",
    "    # Encoded labels\n",
    "    encoded_correct_labels = mlb.transform([correct_labels])\n",
    "    encoded_labels_to_include = mlb.transform([intersection_labels])     \n",
    "\n",
    "    final_classified_test_cases.loc[index_add] = [test_key_nli, test_name_nli, correct_labels, intersection_labels, encoded_correct_labels, encoded_labels_to_include]\n",
    "    index_add += 1\n",
    "\n",
    "    analyzed_key.append(test_key_nli)\n",
    "\n",
    "\n",
    "with mlflow.start_run(experiment_id=experiment_active.experiment_id, run_name=run_name):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for index,row in final_classified_test_cases.iterrows():\n",
    "        label_encoded = row['encoded_correct_labels']\n",
    "        predicted_label_encoded = row['encoded_pred_labels']\n",
    "        y_true.append(label_encoded[0])\n",
    "        y_pred.append(predicted_label_encoded[0])\n",
    "\n",
    "    # Get metrics\n",
    "    metrics = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    precision = metrics[0]\n",
    "    recall = metrics[1]\n",
    "    fscore = metrics[2]\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    duration = \"{0:.2f}\".format(duration)\n",
    "\n",
    "    mlflow.log_param(\"Embedding similarity threshold\", optimal_sim_thresh_embedding)\n",
    "    mlflow.log_param(\"NLI similarity threshold\", optimal_sim_thresh_nli)\n",
    "    mlflow.log_param(\"NLI cross-encoder similarity threshold\", optimal_sim_thresh_nli_cross_enc)\n",
    "    mlflow.log_param(\"Execution time\", duration)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"fscore\", fscore) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b089eb2-ea66-422b-a2e0-c6eab58c2e90",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### EnsBackOffComplete - Ensemble with back-off using all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46115342-e7a7-4f37-b997-581456bcbbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define name and description of experiment\n",
    "experiment_name = \"Ensemble with back-off using all the models - Test case name and objective\"\n",
    "experiment_active = mlflow.set_experiment(experiment_name)\n",
    "experiment_id = experiment_active.experiment_id\n",
    "MlflowClient().set_experiment_tag(experiment_id, \n",
    "     \"mlflow.note.content\",\"Evaluate how the ensemble with back-off using all the models performs for test case name and objective.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb7d5b5-07a9-4d8d-bf1a-a8a5bdb758c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(experiment_dir)\n",
    "experiment_active = mlflow.set_experiment(experiment_name)\n",
    "run_name = \"Test case name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785da400-e3dd-4115-b05c-7bbd9079de99",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use optimal similarity thresholds as found in the experiments with individual models\n",
    "optimal_sim_thresh_embedding = 0.2\n",
    "optimal_sim_thresh_nli = 0.9\n",
    "optimal_sim_thresh_nli_cross_enc = 0.6\n",
    "\n",
    "# Dataframe to store embedding predictions\n",
    "classified_test_cases_embedding = pd.DataFrame(columns=['test_key', 'test_name', 'labels'])\n",
    "\n",
    "# Dataframe to store NLI predictions\n",
    "classified_test_cases_nli = pd.DataFrame(columns=['test_key', 'test_name', 'labels'])\n",
    "\n",
    "# Dataframe to store cross-encoder NLI predictions\n",
    "classified_test_cases_nli_cross_enc = pd.DataFrame(columns=['test_key', 'test_name', 'labels'])\n",
    "\n",
    "index_add = 0\n",
    "analyzed_key = []\n",
    "\n",
    "for index in range(len(test_case_name_df)):\n",
    "    row = test_case_name_df.iloc[index,:]     \n",
    "    test_key = row['id']\n",
    "    test_description = row['description']\n",
    "    correct_labels = row['labels']\n",
    "\n",
    "    if test_key in analyzed_key:\n",
    "        continue\n",
    "\n",
    "    # Get predictions with latent embeddings\n",
    "    sentence_embedding_transformed = classif_res_list_SBERT_name[index]\n",
    "\n",
    "    labels_to_include = set()\n",
    "    \n",
    "    # Iterate through candidate labels and the modified candidate label embeddings\n",
    "    for label, label_embedding in zip(candidate_labels, candidate_label_embeddings_mod_transformed):\n",
    "        cos_sim = util.cos_sim(sentence_embedding_transformed, label_embedding)\n",
    "        if cos_sim > optimal_sim_thresh_embedding:\n",
    "            labels_to_include.add(label)\n",
    "\n",
    "    classified_test_cases_embedding.loc[index_add] = [test_key, test_description, labels_to_include]\n",
    "\n",
    "\n",
    "    # Get predictions from NLI model\n",
    "    classif_res = classif_res_list_BartLargeMNLI_name[index]\n",
    "\n",
    "    # Check we are looking at the correct instance\n",
    "    if test_description != classif_res['sequence']:\n",
    "        print(\"Error!\")\n",
    "        break\n",
    "\n",
    "    predicted_labels = classif_res['labels']\n",
    "    prediction_scores = classif_res['scores']\n",
    "\n",
    "    # Iterate through predictions (descendent order by score)\n",
    "    labels_to_include = set()\n",
    "    scores_to_include = []\n",
    "    for label,score in zip(predicted_labels, prediction_scores):\n",
    "        if score < optimal_sim_thresh_nli:\n",
    "            break\n",
    "        else:\n",
    "            labels_to_include.add(label)\n",
    "            scores_to_include.append(score) \n",
    "\n",
    "    classified_test_cases_nli.loc[index_add] = [test_key, test_description, labels_to_include]\n",
    "\n",
    "\n",
    "    # Get predictions from cross-encoder NLI model\n",
    "    classif_res = classif_res_list_CrossEncoderNLI_name[index]\n",
    "\n",
    "    # Check we are looking at the correct instance\n",
    "    if test_description != classif_res['sequence']:\n",
    "        print(\"Error!\")\n",
    "        break\n",
    "\n",
    "    predicted_labels = classif_res['labels']\n",
    "    prediction_scores = classif_res['scores']\n",
    "\n",
    "    # Iterate through predictions (descendent order by score)\n",
    "    labels_to_include = set()\n",
    "    scores_to_include = []\n",
    "    for label,score in zip(predicted_labels, prediction_scores):\n",
    "        if score < optimal_sim_thresh_nli_cross_enc:\n",
    "            break\n",
    "        else:\n",
    "            labels_to_include.add(label)\n",
    "            scores_to_include.append(score) \n",
    "\n",
    "    classified_test_cases_nli_cross_enc.loc[index_add] = [test_key, test_description, labels_to_include]\n",
    "    index_add += 1\n",
    "    analyzed_key.append(test_key)\n",
    "\n",
    "# Dataframe to store combination of predictions\n",
    "final_classified_test_cases = pd.DataFrame(columns=['test_key', 'test_name', 'correct_labels', 'pred_labels', 'encoded_correct_labels', 'encoded_pred_labels'])\n",
    "index_add = 0\n",
    "\n",
    "analyzed_key = []\n",
    "\n",
    "for index in range(len(classified_test_cases_nli)):\n",
    "    row = classified_test_cases_nli.iloc[index,:]\n",
    "    test_key_nli = row['test_key']\n",
    "    test_name_nli = row['test_name']\n",
    "    labels_nli = row['labels']\n",
    "\n",
    "    if test_key_nli in analyzed_key:\n",
    "        continue\n",
    "\n",
    "    row = classified_test_cases_nli_cross_enc.iloc[index,:]\n",
    "    test_key_nli_cross_enc = row['test_key']\n",
    "    test_name_nli_cross_enc = row['test_name']\n",
    "    labels_nli_cross_enc = row['labels']\n",
    "\n",
    "\n",
    "    row_emb = classified_test_cases_embedding.iloc[index,:]\n",
    "    test_key_emb = row_emb['test_key']\n",
    "    test_name_emb = row_emb['test_name']\n",
    "    labels_emb = row_emb['labels']\n",
    "\n",
    "    if (test_key_nli != test_key_nli_cross_enc) or (test_key_nli != test_key_emb):\n",
    "        print(\"Error occurred!\")\n",
    "        break\n",
    "\n",
    "    intersection_labels_nli = labels_nli.intersection(labels_nli_cross_enc)\n",
    "    intersection_labels = intersection_labels_nli.intersection(labels_emb)\n",
    "\n",
    "    if len(intersection_labels) == 0:\n",
    "        intersection_labels = labels_emb.intersection(labels_nli)\n",
    "\n",
    "        if len(intersection_labels) == 0:\n",
    "            intersection_labels = labels_emb.intersection(labels_nli_cross_enc)\n",
    "\n",
    "            if len(intersection_labels) == 0:\n",
    "                intersection_labels = labels_nli.intersection(labels_nli_cross_enc)\n",
    "\n",
    "    correct_labels = test_case_name_df.iloc[index,3]\n",
    "\n",
    "    # Encoded labels\n",
    "    encoded_correct_labels = mlb.transform([correct_labels])\n",
    "    encoded_labels_to_include = mlb.transform([intersection_labels])     \n",
    "\n",
    "    final_classified_test_cases.loc[index_add] = [test_key_nli, test_name_nli, correct_labels, intersection_labels, encoded_correct_labels, encoded_labels_to_include]\n",
    "    index_add += 1\n",
    "\n",
    "    analyzed_key.append(test_key_nli)\n",
    "\n",
    "\n",
    "with mlflow.start_run(experiment_id=experiment_active.experiment_id, run_name=run_name):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for index,row in final_classified_test_cases.iterrows():\n",
    "        label_encoded = row['encoded_correct_labels']\n",
    "        predicted_label_encoded = row['encoded_pred_labels']\n",
    "        y_true.append(label_encoded[0])\n",
    "        y_pred.append(predicted_label_encoded[0])\n",
    "\n",
    "    # Get metrics\n",
    "    metrics = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    precision = metrics[0]\n",
    "    recall = metrics[1]\n",
    "    fscore = metrics[2]\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    duration = \"{0:.2f}\".format(duration)\n",
    "\n",
    "    mlflow.log_param(\"Embedding similarity threshold\", optimal_sim_thresh_embedding)\n",
    "    mlflow.log_param(\"NLI similarity threshold\", optimal_sim_thresh_nli)\n",
    "    mlflow.log_param(\"NLI cross-encoder similarity threshold\", optimal_sim_thresh_nli_cross_enc)\n",
    "    mlflow.log_param(\"Execution time\", duration)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"fscore\", fscore) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c06bb2-fb47-4882-8435-fcfb1cae1a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(experiment_dir)\n",
    "experiment_active = mlflow.set_experiment(experiment_name)\n",
    "run_name = \"Test case name + objective\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266c198e-a21f-46cc-850d-39ba464afdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use optimal similarity thresholds as found in the experiments with individual models\n",
    "optimal_sim_thresh_embedding = 0.2\n",
    "optimal_sim_thresh_nli = 0.9\n",
    "optimal_sim_thresh_nli_cross_enc = 0.6\n",
    "\n",
    "# Dataframe to store embedding predictions\n",
    "classified_test_cases_embedding = pd.DataFrame(columns=['test_key', 'test_name', 'labels'])\n",
    "\n",
    "# Dataframe to store NLI predictions\n",
    "classified_test_cases_nli = pd.DataFrame(columns=['test_key', 'test_name', 'labels'])\n",
    "\n",
    "# Dataframe to store cross-encoder NLI predictions\n",
    "classified_test_cases_nli_cross_enc = pd.DataFrame(columns=['test_key', 'test_name', 'labels'])\n",
    "\n",
    "index_add = 0\n",
    "analyzed_key = []\n",
    "\n",
    "for index in range(len(test_case_name_obj_df)):\n",
    "    row = test_case_name_df.iloc[index,:]     \n",
    "    test_key = row['id']\n",
    "    test_description = row['description']\n",
    "    correct_labels = row['labels']\n",
    "\n",
    "    if test_key in analyzed_key:\n",
    "        continue\n",
    "\n",
    "    # Get predictions with latent embeddings\n",
    "    sentence_embedding_transformed = classif_res_list_SBERT_name_obj[index]\n",
    "\n",
    "    labels_to_include = set()\n",
    "    \n",
    "    # Iterate through candidate labels and the modified candidate label embeddings\n",
    "    for label, label_embedding in zip(candidate_labels, candidate_label_embeddings_mod_transformed):\n",
    "        cos_sim = util.cos_sim(sentence_embedding_transformed, label_embedding)\n",
    "        if cos_sim > optimal_sim_thresh_embedding:\n",
    "            labels_to_include.add(label)\n",
    "\n",
    "    classified_test_cases_embedding.loc[index_add] = [test_key, test_description, labels_to_include]\n",
    "\n",
    "    \n",
    "    # Get predictions from NLI model\n",
    "    classif_res = classif_res_list_BartLargeMNLI_name_obj[index]\n",
    "\n",
    "    # Check we are looking at the correct instance\n",
    "    if test_description != classif_res['sequence']:\n",
    "        print(\"Error!\")\n",
    "        break\n",
    "\n",
    "    predicted_labels = classif_res['labels']\n",
    "    prediction_scores = classif_res['scores']\n",
    "\n",
    "    # Iterate through predictions (descendent order by score)\n",
    "    labels_to_include = set()\n",
    "    scores_to_include = []\n",
    "    for label,score in zip(predicted_labels, prediction_scores):\n",
    "        if score < optimal_sim_thresh_nli:\n",
    "            break\n",
    "        else:\n",
    "            labels_to_include.add(label)\n",
    "            scores_to_include.append(score) \n",
    "\n",
    "    classified_test_cases_nli.loc[index_add] = [test_key, test_description, labels_to_include]\n",
    "\n",
    "\n",
    "    # Get predictions from cross-encoder NLI model\n",
    "    classif_res = classif_res_list_CrossEncoderNLI_name_obj[index]\n",
    "\n",
    "    # Check we are looking at the correct instance\n",
    "    if test_description != classif_res['sequence']:\n",
    "        print(\"Error!\")\n",
    "        break\n",
    "\n",
    "    predicted_labels = classif_res['labels']\n",
    "    prediction_scores = classif_res['scores']\n",
    "\n",
    "    # Iterate through predictions (descendent order by score)\n",
    "    labels_to_include = set()\n",
    "    scores_to_include = []\n",
    "    for label,score in zip(predicted_labels, prediction_scores):\n",
    "        if score < optimal_sim_thresh_nli_cross_enc:\n",
    "            break\n",
    "        else:\n",
    "            labels_to_include.add(label)\n",
    "            scores_to_include.append(score) \n",
    "\n",
    "    classified_test_cases_nli_cross_enc.loc[index_add] = [test_key, test_description, labels_to_include]\n",
    "    index_add += 1\n",
    "    analyzed_key.append(test_key)\n",
    "\n",
    "# Dataframe to store combination of predictions\n",
    "final_classified_test_cases = pd.DataFrame(columns=['test_key', 'test_name', 'correct_labels', 'pred_labels', 'encoded_correct_labels', 'encoded_pred_labels'])\n",
    "index_add = 0\n",
    "\n",
    "analyzed_key = []\n",
    "\n",
    "for index in range(len(classified_test_cases_nli)):\n",
    "    row = classified_test_cases_nli.iloc[index,:]\n",
    "    test_key_nli = row['test_key']\n",
    "    test_name_nli = row['test_name']\n",
    "    labels_nli = row['labels']\n",
    "\n",
    "    if test_key_nli in analyzed_key:\n",
    "        continue\n",
    "\n",
    "    row = classified_test_cases_nli_cross_enc.iloc[index,:]\n",
    "    test_key_nli_cross_enc = row['test_key']\n",
    "    test_name_nli_cross_enc = row['test_name']\n",
    "    labels_nli_cross_enc = row['labels']\n",
    "\n",
    "\n",
    "    row_emb = classified_test_cases_embedding.iloc[index,:]\n",
    "    test_key_emb = row_emb['test_key']\n",
    "    test_name_emb = row_emb['test_name']\n",
    "    labels_emb = row_emb['labels']\n",
    "\n",
    "    if (test_key_nli != test_key_nli_cross_enc) or (test_key_nli != test_key_emb):\n",
    "        print(\"Error occurred!\")\n",
    "        break\n",
    "\n",
    "    intersection_labels_nli = labels_nli.intersection(labels_nli_cross_enc)\n",
    "    intersection_labels = intersection_labels_nli.intersection(labels_emb)\n",
    "\n",
    "    if len(intersection_labels) == 0:\n",
    "        intersection_labels = labels_emb.intersection(labels_nli)\n",
    "\n",
    "        if len(intersection_labels) == 0:\n",
    "            intersection_labels = labels_emb.intersection(labels_nli_cross_enc)\n",
    "\n",
    "            if len(intersection_labels) == 0:\n",
    "                intersection_labels = labels_nli.intersection(labels_nli_cross_enc)\n",
    "\n",
    "    correct_labels = test_case_name_obj_df.iloc[index,3]\n",
    "\n",
    "    # Encoded labels\n",
    "    encoded_correct_labels = mlb.transform([correct_labels])\n",
    "    encoded_labels_to_include = mlb.transform([intersection_labels])     \n",
    "\n",
    "    final_classified_test_cases.loc[index_add] = [test_key_nli, test_name_nli, correct_labels, intersection_labels, encoded_correct_labels, encoded_labels_to_include]\n",
    "    index_add += 1\n",
    "\n",
    "    analyzed_key.append(test_key_nli)\n",
    "\n",
    "\n",
    "with mlflow.start_run(experiment_id=experiment_active.experiment_id, run_name=run_name):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for index,row in final_classified_test_cases.iterrows():\n",
    "        label_encoded = row['encoded_correct_labels']\n",
    "        predicted_label_encoded = row['encoded_pred_labels']\n",
    "        y_true.append(label_encoded[0])\n",
    "        y_pred.append(predicted_label_encoded[0])\n",
    "\n",
    "    # Get metrics\n",
    "    metrics = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    precision = metrics[0]\n",
    "    recall = metrics[1]\n",
    "    fscore = metrics[2]\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    duration = \"{0:.2f}\".format(duration)\n",
    "\n",
    "    mlflow.log_param(\"Embedding similarity threshold\", optimal_sim_thresh_embedding)\n",
    "    mlflow.log_param(\"NLI similarity threshold\", optimal_sim_thresh_nli)\n",
    "    mlflow.log_param(\"NLI cross-encoder similarity threshold\", optimal_sim_thresh_nli_cross_enc)\n",
    "    mlflow.log_param(\"Execution time\", duration)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"fscore\", fscore) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-22.08",
   "language": "python",
   "name": "rapids-22.08"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
